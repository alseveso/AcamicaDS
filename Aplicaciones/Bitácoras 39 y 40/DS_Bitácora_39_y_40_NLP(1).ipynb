{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbI1F_lNbzLS"
   },
   "source": [
    "# NLP\n",
    "\n",
    "A lo largo del notebook vamos a trabajar con el siguiente dataset:\n",
    "\n",
    "https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection/\n",
    "\n",
    "En esa competencia de Kaggle también puedes encontrar Notebooks (*kernels*) sumamente interesantes.\n",
    "\n",
    "El objetivo es que se familiaricen con algunas herramientas típicas del Procesamiento del Lenguaje Natural (NLP por sus siglas en inglés).La biblioteca fundamental que vamos a usar es NLTK. Probablemente tengas que instalarla. Para ello, googlea cómo instalar esta librería con Conda (no olvides activar el ambiente).\n",
    "\n",
    "Ten en cuenta que la mayoría de las herramientas de NLP pueden ser consideradas como parte del preprocesamiento, llevar el texto a una forma que la computadora pueda entender. En general, esto corresponde a una forma tipo \"tabla\" al estilo de los DataFrames de Pandas. Es fácil olvidarse de ese objetivo y perderse en todos los pasos. Al principio es común marearse, pero no te preocupes que verás que no es difícil.\n",
    "\n",
    "\n",
    "## 1. Carga de datos\n",
    "\n",
    "Lo primero que tienes que hacer es fijarse en qué formato están los datos. ¿De qué se trata es formato?¿Cómo se abre? Si googleas, vas a ver que hay muchas formas de abrir archivos JSON con Python. Como venimos trabajando con Pandas, googlea \"Open JSON with Pandas\". Prueben esa función. Si les tira un error en el primer intento, googleen el error. Les aseguramos que la respuesta está muy a mano y es muy accesible, no tienen que hacer nada raro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLzOjA76bzLW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Esto sirve para configurar NLTK. La primera vez puede tardar un poco\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kt3-hjIxbzLm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ePc6hmJbzL4"
   },
   "source": [
    "¿Qué forma tiene el dataset?¿Cuántas instancias?¿Cuáles son sus columnas?¿Cuántos titulares hay de cada tipo?¿Podemos hablar ya de *features*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMy-ge1dbzL6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14985</td>\n",
       "      <td>14985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13634</td>\n",
       "      <td>13634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              headline  article_link\n",
       "is_sarcastic                        \n",
       "0                14985         14985\n",
       "1                13634         13634"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('is_sarcastic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZeU9RGcbzMH"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=dataset, x = 'is_sarcastic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXqEalYYbzMQ"
   },
   "source": [
    "## 2. Breve exploración del dataset\n",
    "\n",
    "Elige una instancia del dataset al azar y selecciona el *headline*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bzCX9jlobzMT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13906 obama spends another night searching behind white house paintings for safes\n"
     ]
    }
   ],
   "source": [
    "index_random = np.random.randint(0, dataset.shape[0])\n",
    "titular = dataset.iloc[index_random].headline\n",
    "print(index_random, titular)\n",
    "#print(index_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXkf3agRbzMe"
   },
   "source": [
    "¿Te parece que es sarcástico?¿Qué características del texto te hace creer - o no - eso? Comprueba si es sarcástico o no imprimiendo en la celda de abajo el valor correspondiente del dataset. (Como la mayoría de los titulares están en inglés y encima refieren a política local, no te preocupes si es una tarea difícil)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5pwbQbZbzMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13906 1\n"
     ]
    }
   ],
   "source": [
    "print(index_random, dataset.iloc[index_random].is_sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSK8pV41bzMp"
   },
   "source": [
    "## 3. NLTK\n",
    "\n",
    "Si es difícil para algunos humanos detectar el sarcasmo, probablemente también lo sea para una computadora. De todas formas, se puede hacer el intento. Para ello, es necesario extraer características de cada texto que nos sirvan para ir apuntando al objetivo. En los videos de Acámica hay muchos ejemplo de herramientas para aplicar. Elegir un titular que les llame la atención y probar las siguientes herramientas:\n",
    "\n",
    "### Tokenización\n",
    "\n",
    "¿Qué es y para qué sirve?¿Cuáles de todas las formas de tokenización presentadas te parece más útil para este problema?\n",
    "\n",
    "1. `sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGaB_X_nbzMr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obama spends another night searching behind white house paintings for safes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titular_st = nltk.sent_tokenize(titular, language ='english')\n",
    "titular_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obama spends another night searching behind white house paintings for safes\n",
      "['obama spends another night searching behind white house paintings for safes']\n"
     ]
    }
   ],
   "source": [
    "print(titular)\n",
    "print(titular_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTIeN4AAbzMy"
   },
   "source": [
    "2. `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYPqCiHVbzM0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obama',\n",
       " 'spends',\n",
       " 'another',\n",
       " 'night',\n",
       " 'searching',\n",
       " 'behind',\n",
       " 'white',\n",
       " 'house',\n",
       " 'paintings',\n",
       " 'for',\n",
       " 'safes']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titular_wt = nltk.word_tokenize(titular, language ='english', preserve_line = False)\n",
    "titular_wt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEFbUpjHbzM9"
   },
   "source": [
    "### Normalización\n",
    "\n",
    "¿Qué es y para qué sirve? Notar que varias formas de normalización ya vienen aplicadas en el dataset.\n",
    "\n",
    "1. Stopwords\n",
    "\n",
    "Importar los `stopwords` del inglés e imprimirlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBgjKBatbzM-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnhvYEiqbzNG"
   },
   "source": [
    "¿Les parece conveniente aplicar todos los stopwords que aparecen en esa lista?\n",
    "\n",
    "Eliminar del titular elegido los stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP7zHuaubzNI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama', 'spends', 'another', 'night', 'searching', 'behind', 'white', 'house', 'paintings', 'safes']\n"
     ]
    }
   ],
   "source": [
    "titular_wt_sin_sw = [w for w in titular_wt if not w in stop_words] \n",
    "  \n",
    "titular_wt_sin_sw = []\n",
    "\n",
    "for w in titular_wt: \n",
    "    if w not in stop_words: \n",
    "        titular_wt_sin_sw.append(w)\n",
    "\n",
    "print(titular_wt_sin_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obama spends another night searching behind white house paintings for safes\n"
     ]
    }
   ],
   "source": [
    "print(titular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdGXNekwbzNR"
   },
   "source": [
    "¿Cuál o cuáles palabras se fueron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiQP71GGbzNS"
   },
   "source": [
    "### Frecuencia de palabras\n",
    "\n",
    "Dado el titular ya tokenizado por palabras y sin stopwords, usar `nltk` para extrar la frecuencia con que aparece cada palabras. ¿Tiene sentido esto para titulares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvKHBovAbzNV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'obama': 1, 'spends': 1, 'another': 1, 'night': 1, 'searching': 1, 'behind': 1, 'white': 1, 'house': 1, 'paintings': 1, 'safes': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = nltk.FreqDist(titular_wt_sin_sw)\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HauGeHN1bzNg"
   },
   "source": [
    "### Dataset Completo\n",
    "\n",
    "Antes de pasar a extraer features de cada instancia del Dataset, podemos hacer un pequeño análisis del dataset en su conjunto. Por ejemplo, una opción es agrupar todos los titulares por tipo y extraer de cada clase las palabras más frecuentes. Para ello:\n",
    "\n",
    "1. Agrupar los titulares por tipo. Crear un dataframe para cada uno. Recuerden usar máscaras.\n",
    "2. Crear una lista vacia y agregar en esa lista todos los titulares (por tipo/dataframe creado) ya tokenizados (usar el `RegexpTokenizer`) y filtrado por `stopwords`.\n",
    "3. Usar el `FreqDist` en esa lista que acaban de llenar. Llevar lo que devuelve `FreqDist` a un Dataframe. Ordenar por frecuencia en que aparece cada palabra.\n",
    "4. Hacer un `barplot` o similar para visualizar.\n",
    "5. ¿Qué palabras filtrarían, aparte de las que aparecen en `stopwords`? Crear una lista vacía y agregarlas a mano. Agregar en el código que realizaron una línea (similar a la que usan con `stopwords`) para que también filtre por esas palabras.\n",
    "6. Volver a visualizar.\n",
    "\n",
    "#### No-Sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McOO8kqtbzNi"
   },
   "outputs": [],
   "source": [
    "dataset_no_sarcasmo = dataset[COMPLETAR]\n",
    "dataset_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxFlSSl7bzNn"
   },
   "outputs": [],
   "source": [
    "todos_titulares_no_sarcasmo = COMPLETAR\n",
    "for i in range(dataset_no_sarcasmo.shape[0]):\n",
    "    titular = COMPLETAR #seleccionar el titular\n",
    "    titular = COMPLETAR # Tokenizar con RegexpTokenizer\n",
    "    titular = COMPLETAR # Filtrar por stopwords\n",
    "    todos_titulares_no_sarcasmo.COMPLETAR(COMPLETAR) #agregar el resultado a la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzG1R9RubzNy"
   },
   "outputs": [],
   "source": [
    "# Esta celda convierte una lista de listas en una unica lista\n",
    "todos_titulares_no_sarcasmo = list(itertools.chain(*todos_titulares_no_sarcasmo))\n",
    "todos_titulares_no_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfxwmHmfbzN5"
   },
   "outputs": [],
   "source": [
    "#FreqDist\n",
    "freq_no_sarcasmo = nltk.COMPLETAR(COMPLETAR)\n",
    "freq_no_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJkSgmKTbzN_"
   },
   "outputs": [],
   "source": [
    "# googlear: how to get pandas dataframe from freqdist\n",
    "df_no_sarcasmo = COMPLETAR\n",
    "df_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppMj2LfjbzOF"
   },
   "outputs": [],
   "source": [
    "# ordenar por frecuencia\n",
    "df_no_sarcasmo.COMPLETAR(COMPLETAR)\n",
    "df_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FD1fS6obzON"
   },
   "outputs": [],
   "source": [
    "df_no_sarcasmo.reset_index(drop = True, inplace=True)\n",
    "df_no_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovO0orrCbzOT"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.barplot(x  = df_no_sarcasmo.iloc[:30].Word, y = df_no_sarcasmo.iloc[:30].Frequency)\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qreUCHfxbzOa"
   },
   "source": [
    "#### Sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xT1hIph3bzOb"
   },
   "outputs": [],
   "source": [
    "dataset_sarcasmo = COMPLETAR\n",
    "dataset_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6dP3izSbzOh"
   },
   "outputs": [],
   "source": [
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHPs0RgGbzOr"
   },
   "outputs": [],
   "source": [
    "todos_titulares_sarcasmo = COMPLETAR\n",
    "todos_titulares_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMVQp7LtbzOw"
   },
   "outputs": [],
   "source": [
    "freq_sarcasmo = COMPLETAR\n",
    "freq_sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbPLeAuxbzO1"
   },
   "outputs": [],
   "source": [
    "df_sarcasmo = COMPLETAR\n",
    "df_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ub6aoiobzO-"
   },
   "outputs": [],
   "source": [
    "df_sarcasmo.COMPLETAR\n",
    "df_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1Db8DWIbzPE"
   },
   "outputs": [],
   "source": [
    "df_sarcasmo.COMPLETAR\n",
    "df_sarcasmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOxxR-qLbzPM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqXh5Jl5bzPQ"
   },
   "outputs": [],
   "source": [
    "filtrar = []\n",
    "if False:\n",
    "    filtrar.append(\"u\")\n",
    "    filtrar.append(\"new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SKcTDo9bzPV"
   },
   "source": [
    "## 4. Stemming\n",
    "Por razones gramaticales muchas palabras pueden escribirse de manera distinta (debido a conjugación, género, número) pero tener el mismo significado para el texto. Por ejemplo si decimos \"jugar\", \"jugando\" o \"juega\", debido a como estan conjugadas, la computadora puede tratarlas como palabras distintas. Pero, en términos de significado, todas estan relacionadas al verbo Jugar. Muchas veces nos va a convenir unir todas estos términos en uno solo.\n",
    "\n",
    "Una de las manera de hacer esto es por \"STEMMING\". El Stemming es un proceso eurístico que recorta la terminación de las palabras, agrupándolas por su raiz. Reduzcamos la cantidad de palabras diferentes en nuestro dataset utilizando este proceso.\n",
    "\n",
    "a. Importar nuevamente el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FGKmVFEbzPX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = COMPLETAR\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Su3WCmGlbzPd"
   },
   "source": [
    "b. Tomar del `dataset` solo las columnas de interes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91gcGHkFbzPf"
   },
   "outputs": [],
   "source": [
    "dataset = COMPLETAR\n",
    "dataset.dropna(axis=0,inplace=True)  # Si hay alguna nan, tiramos esa instancia\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbzW1O41bzPi"
   },
   "source": [
    "c. Antes de realizar el proceso de Stemming, vamos a normalizar el texto de la manera que ya estuvimos viendo. Le agregamos en este caso el uso de la libreria `re`, que nos permite sacar del texto todos los caracteres que no sean palabras. Notemos que hay veces que no conviene quitar estos caracteres ya que, por ejemplo, no podremos distiguir preguntas (?) o exclamaciones (!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmCJ7RR4bzPj"
   },
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttarTPHhbzPn"
   },
   "outputs": [],
   "source": [
    "# Recorremos todos los titulos y le vamos aplicando la Normalizacion y luega el Stemming a cada uno\n",
    "titular_list=[]\n",
    "for titular in dataset.headline:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular=re.sub(\"[^a-zA-Z]\",\" \",str(titular))\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular=titular.lower()\n",
    "    # Tokenizamos para separar las palabras del titular\n",
    "    titular=nltk.COMPLETAR\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    titular = [palabra for palabra in titular if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    titular = [COMPLETAR for COMPLETAR in COMPLETAR if not COMPLETAR in COMPLETAR]\n",
    "    \n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "    \n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    titular = [stemmer.stem(COMPLETAR) for COMPLETAR in COMPLETAR]\n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular = \" \".join(titular)\n",
    "    \n",
    "    # Vamos armando una lista con todos los titulares\n",
    "    titular_list.append(COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdS9znBvbzPw"
   },
   "source": [
    "d. Agregamos al dataset una columna llamado `titular_stem` que contenga los titulares stemmizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9-UZChubzPw"
   },
   "outputs": [],
   "source": [
    "dataset[\"titular_stem\"] = COMPLETAR\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JN7C6bc6bzP2"
   },
   "source": [
    "e. Armamos un nuevo dataset llamado `dataset_stem` que contenga solo las columnas `titular_stem` e `is_sarcastic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOlCrkdNbzP2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_stem = COMPLETAR\n",
    "dataset_stem.dropna(axis=0,inplace=True)  # Por si quedaron titulares vacios\n",
    "dataset_stem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzwneqyGbzP7"
   },
   "source": [
    "## 5. Lemmatization\n",
    "\n",
    "Otra manera de llevar distintas palabras a un raíz común en la que comparten un significado es mediante el procezo de 'Lemmatizar' el texto. Es similar al 'Stemming' pero un poco más educado, ya que intenta realizar el proceso teniendo en cuenta cuál es el rol que la palabra cumple en el texto. Esto quiere decir que su accionar será distinto si la palabra a lemmantizar está actuando como verbo, sustantivo, etc. \n",
    "\n",
    "Para usar las funciones que ofrece `nltk` para lemmantizar, tendremos primero que descargar la libreria `Wordnet` que se encuentra en la solapa 'corpora' y las librerias 'maxent_treebank_pos_' y 'averaged_perceptron_tagger' que se encuentra en la solapa 'Models'. Para eso ejecute la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BD_wZBAfbzP8"
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1eOMhj4bzQA"
   },
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJkxx1mKbzQD"
   },
   "source": [
    "Veamos cómo actúa el lemmatizer sobre una frase de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4hTfRY_bzQF"
   },
   "outputs": [],
   "source": [
    "# Oracion que usaremos como ejemplo\n",
    "frase = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# Hay que regularizar el texto. Dejar solo letra, pasar a minúsculas y tokenizar:\n",
    "\n",
    "# Sacamos todo lo que no sean letras\n",
    "frase = COMPLETAR\n",
    "# Pasamos a minúsculas\n",
    "frase = COMPLETAR\n",
    "# Tokenizamos\n",
    "frase_tokens = COMPLETAR\n",
    "\n",
    "# Veamos como cambians las palabras al lemmatizar\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for palabra in frase_tokens:\n",
    "    print (\"{0:20}{1:20}\".format(palabra,wordnet_lemmatizer.lemmatize(palabra)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBTEwJyEbzQI"
   },
   "source": [
    "¿Te sorprende lo que paso? No cambiaron casi nada (solo se fueron las \"s\" del final). El problema es que precisamos darle información extra al Lemmatizer, decirle qué rol está cumpliendo la palabra en la oración. Si se fijan en la documentación, esto se hace pasandole un argumento extra a la función llamado POS (Part Of Speech).\n",
    "\n",
    "Hay distintos metodos que intentan averiguar el rol que cumple una palabra en una oración. Nosotros vamos a utilizar uno que viene incorporado en NLTK llamado pos_tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uv6lljv6bzQJ"
   },
   "outputs": [],
   "source": [
    "print(nltk.pos_tag(frase_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_1E59WvbzQM"
   },
   "source": [
    "Las etiquetas refieren al tipo de palabra. Vamos a definir una función para traducir estas etiquetas a los valores de POS que entiende `wordnet_lemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amOOlIfjbzQR"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVkhobaGbzQV"
   },
   "source": [
    "Veamos finalmente como funciona en nuestro ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQ97htEEbzQV"
   },
   "outputs": [],
   "source": [
    "frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(frase)]\n",
    "tipo_palabra = [get_wordnet_pos(w) for w in nltk.word_tokenize(frase)]\n",
    "\n",
    "# Veamos como cambiaron las palabras\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Palabra:\",\"Tipo:\",\"Lemma:\"))\n",
    "for i in range(len(frase_tokens)):\n",
    "    print (\"{0:20}{1:20}{2:20}\".format(frase_tokens[i],tipo_palabra[i],frase_lemma[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6v_x0xknbzQa"
   },
   "source": [
    "a. Ahora te toca aplicar todo esto a nuestro dataset. Vamos a volver a importarlo y hacer un procedimiento análogo al que hicimos para la parte de Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RV9BHH0bzQa"
   },
   "outputs": [],
   "source": [
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "dataset = COMPLETAR\n",
    "dataset = COMPLETAR\n",
    "dataset.dropna(axis=0,inplace=True)\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU1hGlJ8bzQi"
   },
   "outputs": [],
   "source": [
    "titular_list=[]\n",
    "for titular in dataset.headline:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    titular = COMPLETAR\n",
    "    # Pasamos todo a minúsculas\n",
    "    titular = COMPLETAR\n",
    "    # Tokenizamos para separar las palabras\n",
    "    titular = COMPLETAR\n",
    "    \n",
    "    # Aplicamos el Lemmatizer (Esto puede tardar un ratito)\n",
    "    frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in titular]\n",
    "    \n",
    "    \n",
    "    # Eliminamos las palabras d emenos de 3 letras\n",
    "    titular = COMPLETAR\n",
    "    # Sacamos las Stopwords\n",
    "    titular = COMPLETAR\n",
    "    \n",
    "    # Por ultimo volvemos a unir el titular\n",
    "    titular=\" \".join(titular)\n",
    "    #dataset[\"titular_normalizado\"] = titular_list\n",
    "    titular_list.append(titular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8tvgWiWbzQl"
   },
   "outputs": [],
   "source": [
    "dataset[\"titular_lemm\"] = COMPLETAR\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvtROY0ZbzQp"
   },
   "source": [
    "b. Por último nos armamos un nuevo datasate llamado `dataset_lem` que tenga solo las columnas `titular_lemm` y `is_sarcastic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wpQVxyabzQq"
   },
   "outputs": [],
   "source": [
    "dataset_lemm = COMPLETAR\n",
    "dataset_lemm.dropna(axis=0,inplace=True)  # Por si quedaron titulares vacios\n",
    "dataset_lemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkpeu0KHbzQw"
   },
   "source": [
    "**Para mirar**: toma algunos titulares al azar y compara su versión `dataset_lemm` y en `dataset_stem`. Explica.\n",
    "\n",
    "## 6. Vectorizar\n",
    "\n",
    "Hasta ahora, tenemos un grupo de palabras por cada titular (bag of words), ya sea *lemmatizado* o *stemmizado*. ¿Te parece que esto servirá para entrenar modelos? La idea ahora es representar esta lista de palabras como un vector. Para esto vamos a utilizar la función `CountVectorizer` de Scikit-Learn. Esta función nos permite representar cada título por un vector con un `1` en las palabras que contiene y un `0` en las que no. Además, vamos a trabajar únicamente con las palabras que aparecen más veces en el texto, ya que las que aparecen una única vez o pocas veces no nos van a brindar información que se pueda generalizar.\n",
    "\n",
    "a. Tomamos la lista de palabras y el vector que nos dice si es o no sarcástico el título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWkx028SbzQx"
   },
   "outputs": [],
   "source": [
    "# Tomamso la lista de palabras y el vector que nos dice si es o no sarcastico el titulo\n",
    "list_titulos = list(COMPLETAR)\n",
    "is_sarc = COMPLETAR\n",
    "\n",
    "## Para probar con Stemmizer:\n",
    "#list_titulos = list(dataset_stem'titular_stem'].values)\n",
    "#is_sarc = dataset_stem['is_sarcastic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6MXrFoPbzQ1"
   },
   "source": [
    "b. Preparamos el conversor de bag of words a vectores que traemos de sklearn. `CountVectorizer` posee varias funcionalidades que pueden determinarse a partir de parámetros. Les recomendamos fuertemente leer su documentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKLFr1AsbzQ2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Usaremos solo las 1000 palabras con mas frecuencia en todo el corpus para generar los vectores\n",
    "max_features=1000\n",
    "\n",
    "# Es decir que cada instancia tendrá 1000 features\n",
    "cou_vec=CountVectorizer(max_features=max_features) # stop_words=\"english\" , ngram_range=(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obq27UunbzQ-"
   },
   "source": [
    "Notemos que desde `CountVectorizer` se pueden quitar las stopwords (algo que ya hicimos con `nltk`) e incluir los n_gramas automáticamente.\n",
    "\n",
    "c. Ahora sí, vamos generarnos los vectores para cada título a partir del corpus total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ9kXfC6bzQ_"
   },
   "outputs": [],
   "source": [
    "matriz_titulos = cou_vec.fit_transform(list_titulos)\n",
    "\n",
    "# Tomamos las palabras\n",
    "all_words = cou_vec.get_feature_names()\n",
    "\n",
    "# Vizualizamos las 50 palabras mas usadas\n",
    "print(\"50 palabras mas usadas: \",all_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erxuV6RjbzRD"
   },
   "source": [
    "## 7. Modelar\n",
    "\n",
    "Ahora sí estamos listos para usar todo nuestro conocimiento de modelos en este set de datos. Tengamos en cuenta que, dependiendo el número de palabras (features) que hayamos elegido, los modelos pueden tardar un rato en entrenarse.\n",
    "\n",
    "a. Primero, como siempre, separamos en test y train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2oqFqjGobzRE"
   },
   "outputs": [],
   "source": [
    "x = matriz_titulos.toarray()\n",
    "y = is_sarc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGEQG_jabzRH"
   },
   "source": [
    "Tambien definimos una función que nos permite plotear los resultados en una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiHOPXdjbzRH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def confusion(ytest,y_pred):\n",
    "    names=[\"No Sarcastico\",\"Sarcastico\"]\n",
    "    cm=confusion_matrix(ytest,y_pred)\n",
    "    f,ax=plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n",
    "    plt.xlabel(\"y_pred\")\n",
    "    plt.ylabel(\"y_true\")\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs8q3SWsbzRP"
   },
   "source": [
    "### Naive Bayes\n",
    "a. Empecemos por un simple Naive Bayes para tener un benchmark de referencia para el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8Xs78fPbzRQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(xtrain,ytrain)\n",
    "\n",
    "print(\"acc : \", COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6I9T0I9bzR1"
   },
   "source": [
    "b. Veamos cómo queda graficada la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZ9t2vTfbzR2"
   },
   "outputs": [],
   "source": [
    "y_pred = COMPLETAR\n",
    "confusion(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqdVnbwGbzR5"
   },
   "source": [
    "### Random Forest\n",
    "a. Veamos cómo funciona un random forest para predecir el sarcasmo de una nota en base a su titular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SU-XThJ7bzR5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "rf = COMPLETAR\n",
    "rf.fit(COMPLETAR)\n",
    "\n",
    "print(\"acc : \", COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMMfl_CAbzR8"
   },
   "source": [
    "b. Grafiquen su matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMGLJzV7bzR8"
   },
   "outputs": [],
   "source": [
    "y_pred = COMPLETAR\n",
    "confusion(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQjOQ9nRbzR_"
   },
   "source": [
    "c. Habíamos dicho que algo muy bueno de Random Forest era poder preguntarle por la importancia de los features que uso para clasificar. Veamos en este caso cuales son las palabras que mayormente determinan el sarcasmo de una nota para este clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xh6WwqZbzSB"
   },
   "outputs": [],
   "source": [
    "# Le preguntamos la importancia de cada feature (cada palabra)\n",
    "importances = rf.feature_importances_\n",
    "# Tomamos la lista de palabras\n",
    "all_words = cou_vec.get_feature_names()\n",
    "columns = all_words\n",
    "\n",
    "# Ordenamos por importnacia y tomamos las 20 primeras\n",
    "indices = np.argsort(importances)[::-1]\n",
    "indices = indices[:20]\n",
    "selected_columns = [columns[i] for i in indices]\n",
    "selected_importances = importances[indices]\n",
    "\n",
    "# Por ultimo graficamos\n",
    "plt.figure(figsize = (15,8))\n",
    "sns.barplot(selected_columns, selected_importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBK4aNpGbzSE"
   },
   "source": [
    "### SVM\n",
    "Por último vamos a utilizar uno de los modelos mas prometedores para este tipo de datos donde el numero de features es comparable al número de instancias: SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqrXu8CXbzSF"
   },
   "source": [
    "a. Entrene un modelo de SVM Lineal y calcule su accuracy para C = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2e3_Nk-bzSG"
   },
   "outputs": [],
   "source": [
    "# Notar que en vez de utilizar SVC, vamos a usar LinearSVC. \n",
    "# Para el Kernel Lineal, esta función es MUCHO mas rapida que la tradicional SVC.\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(C = 1)\n",
    "svc.fit(COMPLETAR)\n",
    "\n",
    "print(\"acc : \", COMPLETAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IF9ig0ibzSJ"
   },
   "source": [
    "b. Grafiquen su matrz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coQMmlsYbzSJ"
   },
   "outputs": [],
   "source": [
    "y_pred = COMPLETAR\n",
    "confusion(ytest,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TADWysxBbzSM"
   },
   "source": [
    "## 7. Explora las posibilidades\n",
    "\n",
    "Si llegaste hasta acá, ya cuentas con todas las herramientas para poder explorar qué sucede con el poder predictivo cuando van cambiando la manera en que procesas y vectorizas el texto. Algunas sugerencias para explorar son las siguientes:\n",
    "\n",
    "1. Prueba con Stemmizar en vez de lemmantizar\n",
    "1. Cambia el numero de features que esta tomando.\n",
    "1. Prueba con TF-IDF.\n",
    "1. Incluye los 2-gramas. ¿Qué era eso?\n",
    "1. Conserve los signos de exclamación y pregunta del texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahoTJEJzbzSM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DS_Bitácora_39_y_40_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
