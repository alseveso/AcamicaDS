{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informe Final Carrera\n",
    "Profundiza y ajusta la resolución de tus proyectos: agrega al menos una fuente de datos o prueba un modelo que hasta el momento no hayas aplicado. Elabora un informe final que describa el proceso de toma de decisiones. Sustenta la razón por la cual usaste las librerías y aplicaste los métodos al dataset. Todos tus hallazgos deberán ser comunicados en función de la problemática y las preguntas que buscabas responder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente Proyecto, se trabajará con un **nuevo caso de estudio**, con una **nueva problemática**, no relacionada con los Proyectos anteriores presentados, por lo que en el repositorio de GitHub podrán encontrar el presente Notebook y el Archivo csv con el que se trabajará."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraude Bancario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se abordará la problemática del **Fraude Bancario**, el cual es un negocio de miles de millones de dólares y está aumentando cada año.  \n",
    "A saber, una organización típica pierde aproximadamente el 5% de sus ingresos anuales debido al fraude.  \n",
    "En un contexto de cambiante panorama bancario global, donde las redes de sucursales se están reduciendo, los volúmenes de pagos digitales aumentan y los pagos se procesan en segundos, los estafadores están encontrando creativamente nuevas formas de robar a los bancos y sus clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a la [Encuesta Global de Fraude Bancario 2019](https://home.kpmg/cr/es/home/tendencias/2019/07/fraude_bancario.html) realizado por KPMG [1], podemos observar, entre los hallazgos principales, los siguientes:\n",
    "* Más de la mitad de los encuestados en todo el mundo experimentaron aumentos tanto en el valor total como en el volumen del fraude externo. El aumento de las tipologías de fraude en todo el mundo desde 2015 hasta 2018 incluye el robo de identidad y la toma de posesión de la cuenta, los ataques cibernéticos, las tarjetas sin fraude y las estafas autorizadas de pago automático;\n",
    "* Los bancos a nivel mundial están viendo una tendencia creciente en las estafas. Los estafadores están manipulando y obligando a los clientes a hacer pagos a ellos, sin pasar por los controles bancarios;\n",
    "* La Banca Abierta se considera un desafío importante en el riesgo de fraude de los bancos, ya que los bancos de todo el mundo se están preparando para abrir sus puertas a terceros para acceder a los datos de sus clientes;\n",
    "* Los estafadores son cada vez más sofisticados y pueden cambiar y adaptar rápidamente sus enfoques. Los bancos deben ser ágiles para responder a las nuevas amenazas y adoptar nuevos enfoques y tecnologías para predecir y prevenir el fraude;\n",
    "* En cada región, los bancos encuestados consideraron que el desafío más importante en el riesgo de fraude son los ciberataques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué aplicar modelos ML para detectar Fraude Bancario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de **Machine Learning** (aprendizaje automático) procesan los datos en bruto (sin procesar), como correos electrónicos o mensajes de texto, y luego aprenden de lo que toman como entrada, volviéndose más inteligentes a lo largo del camino.  \n",
    "Por otro lado, los **métodos basados en Reglas** no pueden detectar ningún patrón nuevo en los datos, ya que sólo siguen un escenario preestablecido, que no incluye patrones de actividad fraudulenta ligeramente modificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BankSim Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A partir de ahora, se buscará detectar las transacciones fraudulentas del conjunto de datos de **BankSim**.  \n",
    "* BankSim es un **simulador de pagos bancarios** basado en agentes, y en una muestra de datos transaccionales agregados, proporcionados por un banco en España.\n",
    "* El objetivo principal de BankSim es la **generación de datos sintéticos** que se pueden utilizar para la investigación de detección de fraudes.\n",
    "* Consiste en pagos de varios clientes hechos en diferentes períodos de tiempo y con diferentes cantidades.  \n",
    "* Para mayor información sobre el conjunto de datos (originalmente nombrado 'bs140513_032310.csv'), se puede consultar la página de [Kaggle](https://www.kaggle.com/ntnu-testimon/banksim1) [2].  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos a seguir serán los siguientes:\n",
    "\n",
    "1. [Análisis Exploratorio de Datos (EDA)](#1.-Análisis-Exploratorio-de-Datos)\n",
    "2. [Preprocesamiento de Datos](#2.-Preprocesamiento-de-Datos)\n",
    "3. [PCA (Principal Component Analysis)](#3.-PCA-(Principal-Component-Analysis))\n",
    "4. [Sobremuestreo (Oversampling) con SMOTE](#4.-Sobremuestreo-(Oversampling)-con-SMOTE)\n",
    "5. [Modelos de Machine Learning](#5.-Modelos-de-Machine-Learning)\n",
    "6. [Comparación del Desempeño de los Modelos Desarrollados](#6.-Comparación-del-Desempeño-de-los-Modelos-Desarrollados)\n",
    "7. [Conclusión](#7.-Conclusión)\n",
    "8. [Recursos](#8.-Recursos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Se importan las librerías__ necesarias para trabajar en la consigna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Para hacer cálculos numéricos con Python usando arrays\n",
    "import pandas as pd # Para manipulación y análisis de datos con Python (por Ej. tablas numéricas)\n",
    "import matplotlib.pyplot as plt # Herramienta de visualización\n",
    "import seaborn as sns # Herramienta de visualización\n",
    "sns.set() # interfaz preferida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Se realiza la carga el dataset__ usando las funcionalidades de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim = pd.read_csv('banksim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El Dataset, cuenta con **594.643 Filas**, y **10 Columnas**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Se realiza una __descripción__ de las características:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *step:* representa el día desde el comienzo de la simulación. Tiene 180 pasos, por lo que la simulación dura virtualmente 6 meses. \n",
    "* *customer:* la identificación del cliente.  \n",
    "* *age:* edad categorizada, las cuales se detallan a continuación:  \n",
    "    * 0: <= 18,  \n",
    "    * 1: 19-25,  \n",
    "    * 2: 26-35,  \n",
    "    * 3: 36-45,  \n",
    "    * 4: 46-55,  \n",
    "    * 5: 56-65,  \n",
    "    * 6: > 65  \n",
    "    * U: Desconocido  \n",
    "* *gender:* género del cliente.  \n",
    "    * E: Enterprise,  \n",
    "    * F: Mujer,  \n",
    "    * M: Hombre,  \n",
    "    * U: Desconocido  \n",
    "* *zipcodeOri:* el código postal de origen/fuente.  \n",
    "* *merchant:* la identificación del comerciante.  \n",
    "* *zipMerchant:* el código postal del comerciante.  \n",
    "* *category:* categoría de la compra. A posteriori se verán con más detalle.  \n",
    "* *amount:* monto (gasto) que implica la compra.  \n",
    "* *fraud:* variable objetivo que muestra si la transacción es fraudulenta **(1)** o no **(0)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Info por Columna:__ Se imprimen los nombres de cada columna, se observan sus valores faltantes y se identifica a qué tipo de dato corresponden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Ninguna columna tiene valores faltantes, lo cual es lógico por tratarse de un dataset sintético.*\n",
    "* *Por ello, es que no será necesario realizar imputación de valores.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Análisis del feature **Fraude**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(banksim['fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data = banksim, x = 'fraud', order = banksim['fraud'].value_counts().index, palette='pastel')\n",
    "ax.set_xticklabels([\"No Fraudulentas\",\"Faudulentas\"])\n",
    "plt.xlabel('Tipo de Transacción');\n",
    "plt.ylabel('Cantidad de Transacciones');\n",
    "plt.title('Cantidad de Transacciones por Tipo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como se observa en el gráfico anterior, los datos de las transacciones en fraudulentas y no, según el recuento de instancias, se encuentran **desequilibrados**, siendo las transacciones no fraudulentas casi imperceptibles a la vista*.\n",
    "* *Para equilibrar el conjunto de datos se pueden realizar **técnicas de sobremuestreo o submuestreo**.*  \n",
    "* *El **sobremuestreo** es aumentar el número de la clase minoritaria, generando nuevas instancias de dicha clase.*  \n",
    "* *El **submuestreo** consiste en reducir el número de instancias de la clase mayoritaria, mediante la selección de puntos aleatorios de la misma hasta que sea igual a la clase minoritaria.*  \n",
    "* *Ambas operaciones tienen algunos **riesgos**: El sobremuestreo creará copias o puntos de datos similares que a veces no serían útiles para el caso de la detección de fraudes porque las transacciones fraudulentas pueden variar. El submuestreo significa que perdimos puntos de datos y, por lo tanto, información.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['fraud'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['fraud'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Vemos que las instancias no fraudulentas representan un 98,8% del total, pudiendo corroborarse el desbalanceo del dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Análisis del **Fraude por Categoría**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se cuenta con 15 tipos de categoría diferentes, siendo la más recurrente `es_transportation`, abarca el 85% de las transacciones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Análisis del **Fraude por Monto o Gasto**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Confirmemos lo expuesto en el punto anterior, comprobando la cantidad de fraude y de no fraude que se ha tramitado.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = banksim[['fraud','amount']].corr()\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15}, cmap='Pastel2')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title('Correlación entre Variables - Pearson')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Existe una correlación positiva entre fraude y monto o gasto insumido en la trasacción, aunque no muy elevada.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *A continuación, se exponen el gasto promedio y el porcentaje de fraude por categoría.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gasto Promedio y Porcentaje de Fraude por Categoría\",banksim.groupby('category')['amount','fraud'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Parece que el ocio (`es_leisure`) y los viajes (`es_travel`) son las categorías más seleccionadas por los defraudadores.*\n",
    "* *Los defraudadores eligieron las categorías en las que la gente **gasta más en promedio**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Para confirmar lo antes expuesto, se crean dos Dataframes diferenciando las transacciones fraudulentas de las que no lo son.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulentas = banksim.loc[banksim.fraud == 1] \n",
    "no_fraudulentas = banksim.loc[banksim.fraud == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([fraudulentas.groupby('category')['amount'].mean(),no_fraudulentas.groupby('category')['amount'].mean(),\\\n",
    "           banksim.groupby('category')['fraud'].mean()*100],keys=[\"Fraudulentas\",\"No Fraudulentas\",\"Porcentaje(%)\"],axis=1,\\\n",
    "          sort=False).sort_values(by=['Porcentaje(%)'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *De acuerdo a lo observado anteriormente, podemos decir que para un determinado tipo de categoría, una transacción fraudulenta tendrá un gasto (`amount`) mucho mayor -unos cuatro veces o más-, que el gasto **promedio** para dicha categoría.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se trazan los box plots de los gastos realizados -de transacciones fraudulentas y no fraudulentas- por tipo de categoría.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.boxplot(data = banksim, x = 'amount', y = 'category', palette='pastel')\n",
    "plt.title(\"Gasto Realizado por Tipo de Categoría\", fontsize=12)\n",
    "plt.xlabel(\"Gasto\",fontsize=12)\n",
    "plt.ylabel(\"Categoría\",fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Las categorías tienen un gasto promedio similar, **entre 0 y 500** o **600** quizás (descartando los valores atípicos muy dispersos).*\n",
    "* *La excepción observable, es la categoría de **viajes**, cuyo gasto se encuentra **muy por encima** de las demás categorías.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data = fraudulentas, x = 'amount', alpha=0.5, label='Fraudulentas',bins=100)\n",
    "plt.hist(data = no_fraudulentas, x = 'amount', alpha=0.5, label='No Fraudulentas',bins=100)\n",
    "plt.title(\"Histograma de Transacciones Fraudulentas y No Fraudulentas\")\n",
    "plt.xlabel('Monto de la Transacción');\n",
    "plt.ylabel('Cantidad de Transacciones');\n",
    "plt.ylim(0,10000)\n",
    "plt.xlim(0,1000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Una vez más podemos ver en el histograma, que las transacciones fradulentas son menos en cantidad pero más en monto.*\n",
    "* *Con montos mayores a 400, ya poderíamos sospechar que hay fraude.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Análisis del **Fraude por Edad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['age'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como vimos anteriormente, las transacciones se encuentran categorizadas por franjas etarias, y la categoría 'U' representa las edades desconocidas.*\n",
    "* *La franjas que realizaron más transacciones, son las 2, 3 y 4 (en ese órden), es decir, aquellos que tienen entre 26 a 55 años, lo cual es lógico, ya que se trata de personas en edad laboral y que además se han adaptado al uso de herramientas tecnológicas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((banksim.groupby('age')['fraud'].mean()*100).reset_index().rename(columns={'age':'Age','fraud' : 'Fraud Percent'}).sort_values(by='Fraud Percent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El fraude **ocurre más en edades iguales y menores de 18 años (0º categoría)**.*\n",
    "* *Puede ser que los estafadores realmente sean jóvenes menores de 18 años, aunque lo más probable es que aquellos que comenten fraude, piensen que habría menos consecuencias si muestran su edad más joven.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Análisis del feature **Pasos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['step'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como se indicó anteriormente, `step` representa el día desde el comienzo de la simulación, abarcando 180 días (o 180 pasos).*\n",
    "* *Por lo visto, la distribución de los datos por cada día parece bastante equilibrado, representando el día con más transacciones (día 175) un 0,6% del total, y el de menos operaciones (día 1) un 0,4%, y mostrando una evolución del número de transacciones del 0,03% mensual.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Análisis del feature **Cliente**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['customer'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Tenemos un total de 4.112 clientes.*\n",
    "* *El cliente que mayor cantidad de transacciones realizó, es 'C1978250683', con 265 operaciones (lo cual representa un 0,04% de total de transacciones).*\n",
    "* *`customer` no parece ser una variable determinante a la hora de establecer si una transacción es fraudulenta o no.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Análisis de la variable **Género**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los géneros Femenino y Masculino son los que más transacciones realizaron, es decir, que son personas físicas las que más operaciones efectuaron, mientras que son pocas las Empresas y mucho menos los géneros desconocidos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Análisis de los **Zip Codes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['zipcodeOri'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['zipMerchant'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como se observa, hay un sólo tipo de Zip Code, tanto de Origen como de Comerciante, por lo que luego, en el preprocesamiento de datos, los descartaremos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Análisis del feature **Comerciante**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(banksim['merchant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(banksim['merchant'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Tenemos un total de 50 comerciantes.*\n",
    "* *El comerciante que mayor cantidad de transacciones recibió, es 'M1823072687', con 299.693 operaciones (50,4% de total de transacciones) y el segundo es 'M348934600', con 205426 operaciones (34,5%).*\n",
    "* *Entre el primer y segundo comerciante con más operaciones, representan el 84,9% del total de transacciones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([fraudulentas.groupby('merchant')['amount'].mean(),no_fraudulentas.groupby('merchant')['amount'].mean(),\\\n",
    "           banksim.groupby('merchant')['fraud'].mean()*100],keys=[\"Fraudulentas\",\"No Fraudulentas\",\"Porcentaje(%)\"],axis=1,\\\n",
    "          sort=False).sort_values(by=['Porcentaje(%)'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Parece ser importante el tipo de comerciante a la hora de establecer si una transacción es o no fraudulenta, de acuerdo al gasto promedio expuesto para cada uno de ellos.*\n",
    "* *Un comerciante que presenta un elevado monto promedio en sus operaciones, podría ser, por ejemplo, una determinada agencia de viajes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descartamos las columnas de **Zip Code** y **Customer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Las 3 variables eliminadas, se consideran poco relevantes para nuestro análisis, de acuerdo a lo expuesto en los puntos anteriores.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_filtrado = banksim.drop(['customer','zipcodeOri','zipMerchant'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_filtrado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Vemos que se eliminaron ambas columnas de Zip Codes y Customer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_filtrado.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Encoders:__ Se realizará la transformación de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los features categóricos serán transformados a valores numéricos.* \n",
    "* *Las categorías a trabajar, son nominales, es decir que no tienen un orden.*\n",
    "* *Lo ideal sería realizar dummies, pero por el tamaño del dataset (más de 500.000 instancias) podría demorarse demasiado.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in banksim_filtrado.columns:\n",
    "    if(banksim_filtrado[col_name].dtype == 'object'):\n",
    "        banksim_filtrado[col_name]= banksim_filtrado[col_name].astype('category')\n",
    "        banksim_filtrado[col_name] = banksim_filtrado[col_name].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_filtrado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_filtrado.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se obtuvo la conversión de tipo de datos esperada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banksim_filtrado.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *De los estadísticos expuestos, se destaca el **valor máximo** de la variable `amount`, el cual dista notablemente del valor medio de dicho feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=banksim_filtrado, hue= 'fraud', vars=['step','age','gender','merchant','category','amount','fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Aplicación del Modelo de Reducción de Dimensionalidad - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se implementará la **técnica de PCA**, ya que es particularmente útil en el tratamiento de datos donde existen múltiples colinealidades entre las características / variables, como ocurre en el presente estudio.*\n",
    "* *El análisis de componentes principales es una técnica matemática utilizada para la reducción de dimensionalidad. Su objetivo es reducir el número de features, conservando la mayor parte de la información original.*\n",
    "* *El **propósito principal** de crear un PCA antes de aplicar cualquier técnica de clasificación, será visualizar en 2D cómo se agrupan las transacciones de fraude y no fraude y si existe una clara separación entre ellas.*\n",
    "* *Así, la aplicación de PCA, se llevará a cabo sin previa aplicación de técnica de sobremuestreo, principalmente para que la reducción de la dimensionalidad, se realice en los datos de entrenamiento principales (orginales) y se logre obtener una real visualización de los datos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como **X** vamos a considerar 6 variables -que serán escaladas- que sirven como predictoras, con el fin de luego reducir su dimensionalidad y como **y** al feature `fraud`.*\n",
    "* *Como se expuso anteriormente, algunas de ellas parece tener fuerte influencia en la determinación si una operación es fraudulente o no (como el gasto y el tipo de categoría), y otras no (como los pasos o el género).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se **seleccionan las variables** predictoras (`X`) y la variable a predecir (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = banksim_filtrado[['step','age','gender','merchant','category','amount']]\n",
    "y = banksim_filtrado[['fraud']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Se **escalan los datos**. Normalizamos las X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler # Escala características usando estadísticas que sean robustas a valores atípicos\n",
    "\n",
    "X_escalado = RobustScaler().fit_transform(X)\n",
    "X_escalado[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se define la **matriz de Covarianza**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de covarianza\n",
    "features = X_escalado.T\n",
    "cov_matrix = np.cov(features)\n",
    "cov_matrix[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En la diagonal de la matriz de covarianzas, tenemos varianzas, y los demás elementos son las covarianzas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Se realiza la **Eigendecomposition**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "valores, vectores = np.linalg.eig (cov_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *A partir de ésto, podemos calcular el porcentaje de varianza explicada (explained variance) por componente principal:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varianzas_explicadas = [] \n",
    "for i in range (len (valores)): \n",
    "    varianzas_explicadas.append (valores [i] / np.sum (valores)) \n",
    " \n",
    "    print (np.sum (varianzas_explicadas), '\\n', varianzas_explicadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El primer valor (penúltima fila) es solo la suma de las varianzas explicadas y debe ser igual a 1. El segundo valor (última fila) es una matriz, que representa el porcentaje de varianza explicada por componente principal.*\n",
    "* *El primer componente principal representa el 62% de la varianza de los datos, el segundo el 32%.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4. **Visualizaciones**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proyectado_1 = X_escalado.dot (vectores.T [0]) \n",
    "proyectado_2 = X_escalado.dot (vectores.T [1])\n",
    "res = pd.DataFrame (proyectado_1, columns = ['PC1']) \n",
    "res ['PC2'] = proyectado_2 \n",
    "res ['Y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Primero se visualiza el conjunto de datos en una dimensión: como una línea (no incluímos a PC2).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot (res ['PC1'], [0] * len (res), hue = res ['Y'], s = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Luego, se exponen los datos en un espacio 2D:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure (figsize = (10, 5)) \n",
    "sns.scatterplot(res['PC1'], res['PC2'], hue=res['Y'], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En ambos gráficos se observa que las variables parecen relativamente fácil de separar.*\n",
    "* *Casi la totalidad de las transacciones No Fraudulentas están concentradas en el PC2, mientras que las transacciones fraudulentas, se encuentran más extendidas en la dimensionalidad del PC1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vemos cómo funciona el **modelo PCA**, con 2 componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Para poder realizar la reducción de dimensionalidad\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_escalado)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los datos transformados se han reducido a una sola dimensión.*\n",
    "* *La reducción de dimensionalidad de PCA, eliminó la información a lo largo del eje o ejes principales menos importantes, dejando solo el componente o componentes de los datos con la mayor varianza, en éste caso, el primer y segundo componente.*\n",
    "* *Éste conjunto de datos de dimensión reducida es, en algunos sentidos, \"lo suficientemente bueno\" para codificar las relaciones más importantes entre los puntos: a pesar de reducir la dimensión de los datos, la relación general entre los puntos de datos se conserva en su mayoría.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Entrenamiento del modelo de Árbol de Decisión, una vez realizada la Reducción de Dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Para dividir los datos en subconjuntos de entrenamiento y prueba\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor # Regresor deL Árbol de Decisión\n",
    "from sklearn.metrics import mean_squared_error # Para cálculo del RMSE\n",
    "\n",
    "lista_rmse_train_dt = []\n",
    "lista_rmse_test_dt = []\n",
    "\n",
    "max_depths = [1,2,3,4,5,6,7,8,9,10,20,25,30,40,50,80]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "\n",
    "    # Se define el modelo con la profundidad deseada\n",
    "    tree_regressor = DecisionTreeRegressor(max_depth = max_depth, random_state=10)\n",
    "    \n",
    "    tree_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = tree_regressor.predict(X_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    \n",
    "    y_test_pred = tree_regressor.predict(X_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    lista_rmse_train_dt.append(rmse_train)\n",
    "    lista_rmse_test_dt.append(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(max_depths, lista_rmse_train_dt,'o-',label='train' )\n",
    "plt.plot(max_depths, lista_rmse_test_dt,'o-',label='test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Max. Profundidad\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title('Curva de Validación - Árbol de Decisión')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresor = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "regresor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = regresor.predict(X_train)\n",
    "y_test_pred = regresor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "print(f'Raíz del error cuadrático medio en Test: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los RMSE tanto para Train como para Test, resultan similares entre sí, y a su vez con buen desempeño.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresor.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La primer característica tiene una mayor importancia relativa a la hora de realizar predicciones (su importancia es del 75%).*\n",
    "* *El mismo posee una buena capacidad de predicción, por el hecho de haber realizado reducción de dimensionalidad, pasando de 6 features a 2 estimadores.*\n",
    "* *No se observa en éste caso sobrejuste, ya que los RMSE obtenidos para Train y Test son similares.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sobremuestreo (Oversampling) con SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En el presente análisis, se llevará a cabo una técnica de sobremuestreo llamada SMOTE [3] (técnica de sobremuestreo de minorías sintéticas).* \n",
    "* *SMOTE creará nuevos puntos de datos de la clase minoritaria utilizando las instancias vecinas, de modo que las muestras generadas no son copias exactas, sino que son similares a las instancias que tenemos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se **seleccionan las variables** predictoras (`X`) y la variable a predecir (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = banksim_filtrado[['step','age','gender','merchant','category','amount']]\n",
    "y = banksim_filtrado[['fraud']]\n",
    "\n",
    "print(X.head(),\"\\n\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y==1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Vemos el desbalanceo de los datos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aplicamos **SMOTE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE # Importamos librería SMOTE, para poder realizar el sobremuestreo de los datos\n",
    "\n",
    "#Transformamos el Dataset\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "y_res = pd.DataFrame(y_res)\n",
    "print(y_res.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Una vez aplicado SMOTE para equilibrar el conjunto de datos, los resultados muestran que tenemos el número exacto de instancias de clase (1 y 0)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Aclaración:__ Los clasificadores basados en Árboles de decisión y los métodos de conjuntos basados en Árboles (RF, XGB) son invariantes al escalado de características (no lo requieren para converger a los errores mínimos), pero aún así, podría ser una buena idea reescalar / estandarizar su datos.*  \n",
    "* *En el presente apartado, se optó por llevar a cabo el modelo de Random Forest, **sin previo escalado de los datos**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se realiza el __Train Test Split__ para dividir el entrenamiento del test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res,test_size=0.3,random_state=42,shuffle=True,stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Se define una función para trazar la __curva ROC_AUC__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La curva ROC-AUC, es una buena forma visual de ver el performance de los modelos de clasificación a confeccionar.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para trazar la curva ROC_AUC\n",
    "\n",
    "def plot_roc_auc(y_test, preds):\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title('ROC curve')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Ratio Verdadero Positivo')\n",
    "    plt.xlabel('Ratio Falso Positivo')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Modelo Benchmark - Árbol de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Clasificador deL Árbol de Decisión\n",
    "from sklearn.metrics import classification_report # Crea un informe de texto que muestra las principales métricas de clasificación\n",
    "from sklearn.metrics import confusion_matrix # Calcula la matriz de confusión para evaluar la precisión de una clasificación\n",
    "from sklearn.metrics import roc_curve, auc # Para visualizar la Curva ROC y el AUC\n",
    "from sklearn.metrics import accuracy_score # Calcula el accuracy de una clasificación (incluído en classification_report)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=10)\n",
    "\n",
    "tree.fit(X_train,y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Reporte de Clasificación para Árbol de Decisión: \\n\", classification_report(y_test, y_pred))\n",
    "print(\"Matriz de Confusión para Árbol de Decisión: \\n\", confusion_matrix(y_test,y_pred))\n",
    "plot_roc_auc(y_test, tree.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Vemos que un modelo simple de Árboles de Decisión, parece tener un buen desempeño, determinado principalmente en los valores obtenidos de las métricas `precision` y `accuracy`*\n",
    "* *El mayor error en el que incurre el modelo, es en los `falsos negativos`, es decir en aquellas transacciones No Fraudulentas, que fueron predichas como Fraudulentas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = tree.feature_importances_\n",
    "columns = X.columns\n",
    "sns.barplot(columns, importances)\n",
    "plt.title('Importancia de cada Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como se observa, la variable más importante a la hora de predecir, es `amount` por lejos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se elije el modelo de ML Random Forest, por ser **muy popular** para la detección de fraudes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Random Forest__ es un tipo de algoritmo de aprendizaje automático supervisado basado en el aprendizaje por conjuntos.*\n",
    "* *El aprendizaje conjunto es un tipo de aprendizaje en el que se combinan diferentes tipos de algoritmos o el mismo algoritmo varias veces para formar un modelo de predicción más potente.*\n",
    "* *El algoritmo de bosque aleatorio combina varios algoritmos del mismo tipo, es decir, varios árboles de decisión , lo que da como resultado un bosque de árboles , de ahí el nombre \"Random Forest\" o Bosque Aleatorio.*\n",
    "* *Dicho algoritmo, al igual que el Árbol de Decisión, es un **modelo de aprendizaje supervisado para clasificación**, aunque también puede usarse para problemas de regresión.*\n",
    "* *No debe dejar de nombrarse su principal desventaja: debido a su complejidad, requieren mucho más tiempo para entrenarse que otros algoritmos comparables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1 Optimización de Hiperparámetros: RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El ajuste de los hiperparámetros puede ser ventajoso para crear un modelo que sea mejor en la clasificación.*\n",
    "* *En el caso de Random Forest, puede no ser necesario, ya que los Bosques Aleatorios ya son muy buenos en la clasificación.*\n",
    "* *Utilizar una búsqueda exhaustiva de GridSearch para elegir los valores de los hiperparámetros también puede llevar mucho tiempo.*\n",
    "* *Sin embargo, en los casos en que sólo hay unos pocos valores potenciales para sus hiperparámetros o cuando su modelo de clasificación inicial no es muy preciso, podría ser una buena idea investigar al menos el efecto de cambiar algunos de los valores de hiperparámetros en su modelo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En el presente estudio, en el que el Dataset con el que trabajamos tiene un gran número de instancias, la aplicación de Gridsearch podría demorar horas, y el resultado de aplicarlo o no sería similar.*\n",
    "* *Es por ello, que **a modo de demostración**, es decir, que no es realmente necesario de aplicar, se realizará una optimización de hiperparámetros a partir de RandomSearch, que también demora (unos 30 minutos aproximadamente), pero es más viable en función de los tiempos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se puede optar por no correr la siguiente celda, ya que, los mejores hiperparámetros devueltos a partir de RandomSearch, fueron considerados en el punto `5.2.2`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_clf_rs = RandomForestClassifier(random_state=42)\n",
    "n_estimators = [50,100,150,200]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [2,4,6,8]\n",
    "\n",
    "param_grid = dict(max_depth=max_depth,max_features=max_features,n_estimators=n_estimators)\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=0)\n",
    "ramdom = RandomizedSearchCV(estimator=rf_clf_rs,param_distributions=param_grid,verbose=1,n_iter=32,cv=kfold,n_jobs=-1)\n",
    "# n_iter=32 --> The total space of parameters is 32 --> máximo de iteraciones que me permite realizar\n",
    "\n",
    "ramdom_result = ramdom.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mejores hiperparametros: \"+str(ramdom_result.best_params_))\n",
    "print(\"Mejor Score: \"+str(ramdom_result.best_score_)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Se entrena el modelo Random Forest con los argumentos obtenidos de RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "rf_clf_rs = RandomForestClassifier(n_estimators=50,max_features='auto',max_depth=8,random_state=42,\n",
    "                                verbose=1,class_weight=\"balanced\",oob_score=True,n_jobs=-1)\n",
    "\n",
    "rf_clf_rs.fit(X_train,y_train.values.ravel())\n",
    "y_test_pred = rf_clf_rs.predict(X_test)\n",
    "\n",
    "print(\"Reporte de Clasificación para Random Forest Classifier: \\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Matriz de Confusión para Random Forest Classifier: \\n\", confusion_matrix(y_test,y_test_pred))\n",
    "plot_roc_auc(y_test, rf_clf_rs.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El desempeño del presente modelo, ha demostrado una notable mejoría en relación al de Árboles de Decisión.*\n",
    "* *El hecho de que en la curva ROC-AUC, el **AUC sea = 1**, expone que el valor diagnóstico es perfecto.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf_clf_rs.predict(X_test)\n",
    "y_train_pred = rf_clf_rs.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy sobre el train set: ', metrics.accuracy_score(y_train, y_train_pred)) # Accuracy Asociado al Train\n",
    "print('Accuracy sobre el test set: ', metrics.accuracy_score(y_test, y_test_pred)) # Accuracy Asociado al Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Además, tenemos en éste caso, **bajo sesgo** y **baja varianza**, ya que el desempeño es bueno tanto en el train como en el test.*\n",
    "* *Ésto es precisamente lo que se busca en un modelo, por lo que el presente escenario es muy favorable, ya que la complejidad resulta ser la justa y necesaria.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_clf_rs.feature_importances_\n",
    "columns = X.columns\n",
    "sns.barplot(columns, importances)\n",
    "plt.title('Importancia de cada Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La variable más importante a la hora de predecir sigue siendo `amount`, aunque ahora adquieren notable relevancia las atributos de `category` y `merchant`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparación del Desempeño de los Modelos de ML Desarrollados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      Modelos      | Precision No Fraude | Precisión Fraude |            Hiperparámetros Utilizados           |\n",
    "|:-----------------:|:-------------------:|:----------------:|:-----------------------------------------------:|\n",
    "| Árbol de Decisión |         0.98        |       0.95       |         max_features=none,max_depth=4           |\n",
    "|   Random Forest   |         0.99        |       0.97       | n_estimators=50,max_features='auto',max_depth=8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En éste caso, fue **Random Forest fue el modelo con mejor desempeño**.*\n",
    "* *Éste resultado, era realmente el esperado, ya que se trata de un algoritmo eficiente y fácil de usar que ofrece un alto rendimiento y precisión en comparación con otros algoritmos.*\n",
    "* *Random Forest, además de aplicar Bagging, también selecciona features al azar, de esa manera descorrelaciona aún más los distintos modelos de árbol creados.*\n",
    "* *Cuenta con una **doble aleatoriedad**: tanto en la selección del valor k de características para cada árbol como en la cantidad de muestras que usa para entrenar cada árbol creado.*\n",
    "* *El algoritmo de bosque aleatorio **no está sesgado** (lo cual coincide con lo observado anteriormente), ya que hay varios árboles y cada árbol se entrena en un subconjunto de datos. Básicamente, el algoritmo de bosque aleatorio se basa en el poder de \"la multitud\"; por lo tanto, se reduce el sesgo general del algoritmo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__¿Cuál es el beneficio de usar Random Forest versus Árboles de Decisión?__\n",
    "* *Los bosques aleatorios son una opción más robusta que un simple árbol de decisión.*\n",
    "* *Como Random Forest consiste en una colección de árboles en un subconjunto aleatorio de características, tiene como predicciones finales, los resultados combinados de esos árboles.*\n",
    "* *Éste algoritmo **es muy estable**. Incluso si se introduce un nuevo punto de datos en el conjunto de datos, el algoritmo general no se ve muy afectado, ya que los nuevos datos pueden afectar a un árbol, pero es muy difícil que afecte a todos los árboles.*\n",
    "* *Random Forest **previene el sobreajuste la mayoría de las veces**, creando subconjuntos aleatorios de las características y construyendo árboles más pequeños usando estos subconjuntos. Después, combina los subárboles de submuestras de características, por lo que no tiende a sobreajustarse a todo su conjunto de características de la manera en que lo hacen los Árboles de Decisión.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En el presente estudio se buscó realizar la Detección de Fraudes en los datos de pago de nuestro banco elegido, denominado Banksim.\n",
    "* Como el conjunto de datos de Fraude del Dataset utilizado, tiene un gran problema de desequilibrio, se realizó una técnica de sobremuestreo llamada SMOTE para generar nuevos puntos de datos de la clase minoritaria y así equilibrar las instancias con transacciones fraudulentas y no fraudulentas.\n",
    "* Para la Detección, se llevaron a cabo distintos modelos de ML, habiendo alcanzado muy buenos resultados con los clasificadores elegidos, entre otras razones, por haberse elegido entre ellos el modelo de Random Forest, el cual es muy popular y efectivo a la hora de detectar Fraudes Bancarios.\n",
    "* La razón por la cual elegí trabajar con 'Fraude Bancario' para mi Trabajo Final, es que por un lado, soy economista, trabajo en un banco en el sector de Riesgo Crediticio y considero muy enriquecedor lograr detectar de forma rápida y precisa, la existencia de fraudes, para evaluar luego si es necesario hacer un mayor seguimiento a ciertos tipos de transacciones, o bloquear las mismas. Por otro lado, es sorprendente la cantidad de ingresos que pierden todo tipo de empresas debido a ello, y es realmente atrapante intentar al menos, poder participar para mejorar dicha situación a futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]. Encuesta Global de Fraude Bancario 2019 - KPMG (Mayo 2019). Obtenido de https://home.kpmg/cr/es/home/tendencias/2019/07/fraude_bancario.html  \n",
    "[2]. Synthetic data from a financial payment system. Obtenido de https://www.kaggle.com/ntnu-testimon/banksim1  \n",
    "[3]. SMOTE: Synthetic Minority Over-sampling Technique. Obtenido de https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
