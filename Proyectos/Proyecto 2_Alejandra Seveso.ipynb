{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO 2 #\n",
    "# Ingeniería de features, Modelos avanzados e Interpretación de modelos\n",
    "## PROYECTO: Análisis de mercado inmobiliario ##\n",
    "### PROBLEMA ###  \n",
    "Recientemente te has incorporado al equipo de Datos de una gran inmobiliaria. La primera tarea que se te asigna es ayudar a los tasadores/as a valuar las propiedades, ya que es un proceso difícil y, a veces, subjetivo. Para ello, propones crear un modelo de Machine Learning que, dadas ciertas características de la propiedad, prediga su precio de venta.\n",
    "### RESUMEN DEL PROYECTO ###\n",
    "Aplica transformación de datos y entrena Modelos Avanzados para desarrollar con mayor profundidad tu modelo de Machine Learning. ¿Qué puedes aprender del problema que estás abordando mediante el estudio de tu propio modelo?\n",
    "### CONSIGNA ###\n",
    "En este proyecto profundizarás lo desarrollado en el proyecto 01 (“Primer modelo de Machine Learning”). El objetivo es aplicar las técnicas incorporadas (Transformación de Datos, Optimización de Hiperparámetros, Modelos Avanzados, etc.) para generar un modelo que tenga un mejor desempeño que el modelo generado en el proyecto anterior. Luego, interpreta ese modelo para responder la siguiente pregunta: ¿qué podemos aprender de nuestro problema estudiando el modelo que generamos?\n",
    "## PARTE A - Transformación de Datos ###\n",
    "\n",
    "Elige cuáles de las siguientes tareas son apropiadas para su dataset. Justifica e implementa:\n",
    "\n",
    "* Detección y eliminación de Outliers\n",
    "\n",
    "* Encoding\n",
    "\n",
    "* Imputación de valores faltantes\n",
    "\n",
    "* Escalado de datos\n",
    "\n",
    "* Generación de nuevas variables predictoras/reducción de dimensionalidad (SVD/PCA).\n",
    "\n",
    "Vuelve a entrenar el modelo implementado en la Entrega 01 - en particular, el árbol de decisión - y evalúa su desempeño a partir del dataset obtenido luego de transformar los datos. ¿Hay una mejora en su desempeño? Sea cual sea la respuesta, intenta explicar a qué se debe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Se importan las librerías__ necesarias para trabajar en la consigna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'rcsetup' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\Ale\\anaconda3\\envs\\ds\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cce2883a5993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ds\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;31m# cbook must import matplotlib only within function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m from matplotlib.cbook import (\n\u001b[0;32m    140\u001b[0m     MatplotlibDeprecationWarning, dedent, get_label, sanitize_sequence)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'rcsetup' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\Ale\\anaconda3\\envs\\ds\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Se realiza la carga el dataset__ usando las funcionalidades de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati = pd.read_csv('DS_Proyecto_01_Datos_Properati.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati.shape # Filas y columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El Dataset, cuenta con **146.660 Filas**, y **19 Columnas**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati.head(3) # Primeras 3 instancias (filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Valores Faltantes:__ se imprimen en pantalla los nombres de las columnas y cuántos valores faltantes hay por columna. En un principio es a mera exposición, ya que por el momento no vamos a descartar ninguno de ellos,ni realizar imputación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati.isnull().sum() # Nombres de las columnas y su cantidad de faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Variables con elementos faltantes:*  \n",
    "    *1. `Superficie Cubierta` **15%** (21.614);*  \n",
    "    *2. `Superficie Total` **14%** (20.527);*  \n",
    "    *3. Latitud y Longitud 7% c/u (10.000 c/u);*  \n",
    "    *4. Baños 4% (6.000).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Tipos de propiedad:__ Se explora cuántos tipos de propiedad hay publicados en el dataset y la cantidad de instancias por cada tipo de propiedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(properati['property_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(properati['property_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En el dataset de Properati se encuentran publicados __10 Tipos de Propiedad__ en la zona geográfica analizada.*  \n",
    "* *Se destacan:*  \n",
    "    *1. `Departamento` con 107.326 unidades __(73%)__;*  \n",
    "    *2. `Casa` con 21.521 (15%);*  \n",
    "    *3. `PH` con 14.298 instancias (10%).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = properati, y = 'property_type', order = properati['property_type'].value_counts().index, palette='pastel')\n",
    "plt.title('Número de Publicaciones por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. __Se visualizan las regiones__ a las cuales pertenecen las publicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (7,2))\n",
    "sns.countplot(y = 'l2', data = properati, order = properati['l2'].value_counts().index, color = '#82B3FF')\n",
    "plt.title('Número de Publicaciones por Zona Urbana de la Provincia de Bs. As.')\n",
    "\n",
    "f, ax = plt.subplots(figsize = (7,5))\n",
    "sns.countplot(y = 'l3', data = properati, order = properati['l3'].value_counts().iloc[:10].index, color = '#957dad')\n",
    "plt.title('Número de Publicaciones por Barrio/Partido de la Provincia de Bs. As')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(properati['l2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(properati['l3'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Las publicaciones son principalmente de `Capital Federal` **(63%)**.*  \n",
    "* *Dentro de CABA, se detaca el barrio de `Palermo` (**14%** s/ CABA), seguido por Almagro, Belgrano, Caballito, Villa Crespo y Recoleta.*  \n",
    "* *Dentro de AMBA, el partido de `Tigre` es el que presenta mayor número de publicaciones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. __Se exponen las Estadísticas Descriptivas__, como ser la tendencia central, la dispersión y la forma de la distribución de un conjunto de datos, excluyendo los NaN valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 2) # Para una mejor visualización, se redujeron los decimales a 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *`Surface Total` y `Surface Covered`: los valores mínimos y máximos obtenidos resultan poco razonables para dichas instancias, ya que como se observa, oscilan entre 10m2 - 193.549m2 y 1m2 - 126.062m2 respectivamente. Al respecto, se observa una `Desviación Estándar Alta`, lo cual indica que los datos se extienden sobre un amplio rango de valores.*\n",
    "* *`Bedrooms = 0`, es coherente, ya que podría tratarse de `Monoambientes`, donde se comparten en un mismo ambiente, living, cocina y dormitorio.*\n",
    "* *`Bathrooms = 0`, es lógica por estar trabajando con Depósitos y Lotes por ejemplo, que pueden no tener baños.*\n",
    "* *`Price`: Los mínimos y máximos distan mucho de la media.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformación de Datos\n",
    "\n",
    "### 2.1 Detección y Eliminación de Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Detección de Outliers__ a través de Boxplots, de las variables `Precio`, `Superficie Total` y `Superficie Cubierta` por considerarlas más relevantes y con mayor dispersión de datos s/ las estadísticas descriptivas previamente expuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati, x = 'property_type', y = 'price')\n",
    "plt.title('Precio en Dólares por Tipo de Propiedad')\n",
    "plt.ticklabel_format(axis = 'y', style = 'plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati, x = 'property_type', y = 'surface_total')\n",
    "plt.title('Superficie Total en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati, x = 'property_type', y = 'surface_covered')\n",
    "plt.title('Superficie Cubierta en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Tanto en `Precio` como en `Superficie Total` se visualizan una gran cantidad de outliers.*\n",
    "* *En el caso de `Precio`, los outliers más distantes de la media (por encima de ella) se observan en `Departamento` y `Otro`.*\n",
    "* *En el caso de `Superficie Total`, los outliers más distantes de la media se observan en `Departamento`, `Lote` y `Otro`.*\n",
    "* *`Superficie Cubierta` presenta outliers principalmente en `Departamento`, `Casa` y `PH`.*\n",
    "* *También se observan valores **Nulos**, lo cual no es consistente.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Eliminación de Outliers:__ se procede a descartar los datos atípicos para cada tipo de propiedad.\n",
    "* *Primero, se realiza un **primer filtro** en el dataset, aplicando **IQR Score**, para la variable `Precio`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_p = properati['price'].quantile(0.25)\n",
    "Q3_p = properati['price'].quantile(0.75)\n",
    "IQR_p = Q3_p - Q1_p\n",
    "print (IQR_p) # Se visualiza el IQR para columna precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_price = properati [~ ((properati['price'] <(Q1_p - 1.5 * IQR_p)) | (properati['price']> (Q3_p + 1.5 * IQR_p)))]\n",
    "properati_2 = mask_price\n",
    "\n",
    "properati_2['price'].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Un **7,5%** de los datos resultaron ser atípicos, de acuerdo a la metodología utilizada (IQR Score).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Luego, se procede a filtrar aquellas instancias en las que la `Superficie Cubierta` sea superior a la `Superficie Total`, ya que en la práctica NO es posible que ésto suceda.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sup = (properati_2['surface_covered'] <= properati_2['surface_total'])\n",
    "properati_3 = properati_2[mask_sup]\n",
    "properati_3[['price','surface_total']].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La muestra se redujo casi un **15%** más.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Análisis:__ se procede a visualizar la nueva distribución de los datos a partir del primer filtro realizado.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_3[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati_3, x = 'property_type', y = 'price', palette= 'pastel')\n",
    "plt.title('Precio en Dólares por Tipo de Propiedad')\n",
    "plt.ticklabel_format(axis = 'y', style = 'plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati_3, x = 'property_type', y = 'surface_total')\n",
    "plt.title('Superficie Total en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati_3, x = 'property_type', y = 'surface_covered')\n",
    "plt.title('Superficie Cubierta en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La distribución de datos de `Precio` expone mayor robustez.*\n",
    "* *Sin embargo, en los casos de `Superficie Total` y `Superficie Cubierta` los valores atípicos siguien distando mucho de la media, oscilando entre 10m2 - 169.000m2 y 1m2 - 126.062m2 respectivamente. La Desviación Estándar sigue siendo Elevada.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se realiza un **segundo filtrado**, con el fin de alcanzar valores razonables en las `Superficies`. Comenzamos con `Superficie Total`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_st = properati_3['surface_total'].quantile(0.25)\n",
    "Q3_st = properati_3['surface_total'].quantile(0.75)\n",
    "IQR_st = Q3_st - Q1_st\n",
    "print (IQR_st) # Se visualiza el IQR para columna precios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_st = properati_3 [~ ((properati_3['surface_total'] <(Q1_st - 1.5 * IQR_st)) | (properati_3['surface_total']> (Q3_st + 1.5 * IQR_st)))]\n",
    "properati_4 = mask_st\n",
    "\n",
    "properati_4['surface_total'].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En ésta ocación, la muesta fue acotada un **7,5% más**, en comparación al dataset original.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Nuevo Análisis:__ a fin de observar la robustez de los datos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_4[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati_4, x = 'property_type', y = 'price', palette= 'pastel')\n",
    "plt.title('Precio en Dólares por Tipo de Propiedad')\n",
    "plt.ticklabel_format(axis = 'y', style = 'plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati_4, x = 'property_type', y = 'surface_total', palette= 'pastel')\n",
    "plt.title('Superficie Total en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.boxplot(data = properati_4, x = 'property_type', y = 'surface_covered', palette= 'pastel')\n",
    "plt.title('Superficie Cubierta en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Ahora sí. Tanto la distribución de datos de `Precio` como de las `Superficies` exponen mayor robustez.*\n",
    "* *Si bien aún se verifican algunos Outliers principalmente en precios, por cantidad y valores que alcanzan, sólo lo serían por tipo de propiedad y no considerando el conjunto total de los datos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Imputación de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_4.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Variables que aún cuentan con elementos faltantes:*  \n",
    "    *1. Latitud y Longitud 6,5% c/u (6.621 c/u);*  \n",
    "    *2. Baños 1,3% (1.345).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Nos enfocaremos en la imputación de datos de `Baños`, ya que en el próximo paso, trabajaremos con Departamento, Casa y PH (a fin de comparar nuestros resultados con los del Protyecto 1), y no es posible que no hayan Baños en dichos tipos de propiedades (como sí podría ocurrir en un Depósito).*\n",
    "* *Respecto a latitud y longitud, no se realizará ningún cambio, ya que no serán utilizadas en éste estudio para determinar el precio de las propiedades por la complejidad de su análisis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "sns.distplot(properati_4['bathrooms']) \n",
    "plt.title('Distribución de la variable Baños')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_bathrooms = properati_4['bathrooms'].median()\n",
    "print(median_bathrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_bathrooms = properati_4['bathrooms'].mode()\n",
    "print(median_bathrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bathrooms = properati_4['bathrooms'].mean() # Ya espuesto con anterioridad\n",
    "print(median_bathrooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Si bien alguna de las propiedades posee 14 baños, dicho valor atípico no está influyendo en las medidas de tendencia central, ya que la media, la moda y la mediana son igual a **1.0**. A continuación, **se procederá a imputar los datos faltantes con dicho valor**.*\n",
    "* *Se agrega que la distribución de sus datos es asímétrica a la derecha (positiva), y la cantidad de valores nulos representa un porcentaje bajo sobre el total. Ésto ocurre por la cantidad de valores nulos observados; sin ellos y sin el inmueble con 14 baños, y dados los valores de la media, mediana y moda (iguales entre sí), sería razonable obserbar una distribución normal de los datos en baños.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_4['bathrooms'] = properati_4['bathrooms'].fillna(median_bathrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pd.isnull(properati_4['property_type']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Todos los valores nulos de `Baños` fueron reemplazados por **1.0**, es decir por el valor de la __media, mediana y moda__.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Selección de la Muestra\n",
    "\n",
    "* *A fin de **comparar los resultados** de los modelos del Proyecto 2 **con los resultantes del Proyecto 01**, utilizaremos la misma muestra, es decir, que nos centraremos en:*\n",
    "    * *Los tipos de propiedad con mayor concentración, `Departamento, Casa y PH` y;*\n",
    "    * *En la región con mayor número de publicaciones, es decir, `Capital Federal`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_5 = properati_4 [(properati_4['l2'] == 'Capital Federal') & ((properati_4['property_type'] == 'Departamento') | (properati_4['property_type'] == 'PH') | (properati_4['property_type'] == 'Casa'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La muestra se redujo en un **8% más**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se exponen nuevamente las Estadísticas Descriptivas, con el Dataset Filtrado.*\n",
    "* *En su revisión, se verifica que se cumpla que la **Superficie Cubierta Mínima sea de 18m2**, basándose en el Nuevo Código de Edificación de CABA, que entró en vigencia el 1° de enero de 2019, y hablitó la construcción de **microambientes mínimos**, de hasta 18 metros cuadrados. Se puede verificar en [Nuevo Código de Edificación](https://www.buenosaires.gob.ar/desarrollourbano/nuevo-codigo-de-edificacion).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_5[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Mejora la distribución de los datos, presentando una reducción notable en el desvío estándar de los mismos.*\n",
    "* *No se verifica el cumplimiento de la `Superficie Cubierta Mínima` de 18m2, por lo que a continuación, lo hacemos cumplir:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sup2 = (properati_5['surface_covered'] >= 18)\n",
    "properati_6 = properati_5[mask_sup2]\n",
    "\n",
    "properati_6['surface_covered'].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El **Dataset Final con el que vamos a trabajar**, representa aprox. un **49% del Dataset Original**.*\n",
    "* *Si bien puede parecer un porcentaje bajo, eran muchos los outliers (como propiedades con Sup. Total de 126.062m2) o mal cargados (como Sup. Total < que Sup. Cubierta).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *A continuación se reflejan las **nuevas distribuciones** para las `variables Superficie Total` y `Precio`:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "sns.distplot(properati_6['price'])\n",
    "plt.title('Distribución de la variable Precio')\n",
    "plt.ticklabel_format(style = 'plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "sns.boxplot(data = properati_6, x = 'property_type', y = 'price', palette= 'pastel')\n",
    "plt.title('Precio en Dólares por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "sns.distplot(properati_6['surface_total']) \n",
    "plt.title('Distribución de la variable Superficie Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "sns.boxplot(data = properati_6, x = 'property_type', y = 'surface_total', palette= 'pastel')\n",
    "plt.title('Superficie Total en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "sns.distplot(properati_6['surface_covered']) \n",
    "plt.title('Distribución de la variable Superficie Cubierta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,5))\n",
    "sns.boxplot(data = properati_6, x = 'property_type', y = 'surface_covered', palette= 'pastel')\n",
    "plt.title('Superficie Cubierta en m2 por Tipo de Propiedad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Tanto en el caso de Superficie como de Precios, se visualizan datos más consistentes.*\n",
    "* *En ambos casos, es clara la distribución de datos con **asimetría positiva (o a la derecha)**, siendo la mayor parte de los precios de las propiedades, menores a U$S 250.000.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data=properati_6, hue= 'property_type', vars=['rooms','bedrooms','bathrooms','surface_total','surface_covered','price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Podemos inferir que tanto Superficie Total como Superficie Cubierta, están altamente correlacionadas con el Precio, no así el resto de las variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Correlaciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Correlaciones Pearson:__ Primero se realiza el estudio de las correlaciones entre las variables `rooms, bedrooms, bathrooms, surface_total, surface_covered` y `price`, con el fin de exponer la existencia de correlaciones lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = properati_6[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].corr()\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15}, cmap= 'Set2')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title('Correlación entre Variables - Pearson')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se observa:*\n",
    "    * *Correlación Alta **(0.93)**, entre `rooms` (ambientes) y `bedrooms` (dormitorios).*\n",
    "    * *Correlación Alta **(0.92)**, entre `surface_covered` (superficie total) y `surface_total` (superficie cubierta).*\n",
    "    * *Además, `surface_covered` y `surface_total`, tiene Correlación Alta y Moderadamente Alta, con todas las demás variables, excepto `bathrooms`.*\n",
    "* *Por su parte, `Price` posee una correlación Moderadamente Alta con `surface_covered` **(0.75)** y `surface_total` **(0.73)**, coincidiendo con lo esperado, respecto a ser las __variables más relevantes para determinar el precio de los inmuebles__.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Correlaciones Spearman:__ Para adicionar información al estudio, se expone la correlación a través del método Spearman, con el fin de visualizar la existencia de correlaciones No lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = properati_6[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].corr(method='spearman')\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15}, cmap= 'Set2')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title('Correlación entre Variables - Spearman')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *No se observan importantes cambios entre ambos tipos de correlaciones.*\n",
    "* *Se mantienen las Altas correlaciones entre `rooms` (ambientes) y `bedrooms` (dormitorios), y entre `surface_covered` (superficie total) y `surface_total` (superficie cubierta).*\n",
    "* `surface_covered` y `surface_total`, incrementan su correlación positiva con`rooms` y `bedrooms`*.\n",
    "* *Por su parte, `Price` (precio) posee una correlación algo mayor con `surface_covered` **(0.79)** y `surface_total` **(0.80)**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Escalado de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Primero, se analiza si algunas de las variables requiere escalado.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='surface_total', y='price', data=properati_6, color = '#82B3FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='surface_covered', y='price', data=properati_6, color = '#82B3FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='rooms', y='price', data=properati_6, color = '#82B3FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='bedrooms', y='price', data=properati_6, color = '#82B3FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='bathrooms', y='price', data=properati_6, color = '#82B3FF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En `Rooms`, `Bedrooms` y `Bathrooms` parece preciso aplicar escalado a sus datos, por observarse en ellos, valores atípicos muy grandes que pueden degradar el rendimiento predictivo de los algoritmos de aprendizaje automático.*\n",
    "* *Igualmente, el escalado de datos, **se realizará para las 5 características** previamente expuestas en los gráficos y para `Precio`, con el fin de **normalizar sus datos**, dentro de un rango particular.*\n",
    "* *Se procede a realizar dicho proceso a través de **RobustScaler**, ya que, a diferencia de StandardScaler, sus estadísticas de centrado y escalado, se basan en percentiles y, por lo tanto, no están influenciadas por unos pocos valores atípicos marginales muy grandes (como en Baños). En consecuencia, el rango resultante de los valores de las características transformadas es mayor que para StandardScaler y son aproximadamente similares.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "properati_scaler = RobustScaler().fit_transform(properati_6[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']])\n",
    "\n",
    "properati_scaler.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_7 = pd.DataFrame(properati_scaler, index = properati_6[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].index,\n",
    "                         columns = properati_6[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los datos que utilizaremos en los siguientes modelos, han sido escalados.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='bathrooms', y='price', data=properati_7, color = '#82B3FF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Teniendo como ejemplo a Baños, la distribución es similar.*\n",
    "* *Sin embargo, los datos de todas las variables elegidas, han sido normalizados tal cual esperábamos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El **Dataset Escalado**, incluye únicamente las columnas de `rooms, bedrooms, bathrooms, surface_total, surface_covered` y `price`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Aplicación de Reducción de Dimensionalidad - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se implementa la técnica de PCA, ya que es particularmente útil en el tratamiento de datos donde existen múltiples - colinealidades entre las características / variables, como ocurre en el presente estudio.*\n",
    "* *El análisis de componentes principales es una técnica matemática utilizada para la reducción de dimensionalidad. Su objetivo es reducir el número de features, conservando la mayor parte de la información original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como **X** vamos a considerar las 5 variables escaladas que sirven como predictoras, con el fin de luego reducir su dimensionalidad y como **y** al Precio.*\n",
    "* *Como se expuso anteriormente, entre ellas existen múltiples correlaciones, y a su vez, algunas de ellas parece tener influencia en la determinación de los precios (como las superficies), y otras no (ambientes, dormitorios y baños).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se **seleccionan las variables** predictoras (`X`) y la variable a predecir (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = properati_7[['surface_total','surface_total','bedrooms','rooms','bathrooms']]\n",
    "y = properati_7[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Se define la **matriz de Covarianza**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de covarianza\n",
    "features = X.T\n",
    "cov_matrix = np.cov(features)\n",
    "cov_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En la diagonal de la matriz de covarianzas, tenemos varianzas, y los demás elementos son las covarianzas.\n",
    "* Los elementos diagonales son idénticos y la matriz es simétrica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Se realiza la **Eigendecomposition**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "valores, vectores = np.linalg.eig (cov_matrix) \n",
    "valores [: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores [: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *A partir de esto, podemos calcular el porcentaje de varianza explicada (explained variance) por componente principal:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varianzas_explicadas = [] \n",
    "for i in range (len (valores)): \n",
    "    varianzas_explicadas.append (valores [i] / np.sum (valores)) \n",
    " \n",
    "    print (np.sum (varianzas_explicadas), '\\n', varianzas_explicadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El primer valor (penúltima fila) es solo la suma de las varianzas explicadas y debe ser igual a 1. El segundo valor (última fila) es una matriz, que representa el porcentaje de varianza explicada por componente principal.*\n",
    "* *El primer componente principal representa el 81% de la varianza de los datos, el segundo el 11%.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4. **Visualizaciones**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proyectado_1 = X.dot (vectores.T [0]) \n",
    "proyectado_2 = X.dot (vectores.T [1])\n",
    "res = pd.DataFrame (proyectado_1, columns = ['PC1']) \n",
    "res ['PC2'] = proyectado_2 \n",
    "res ['Y'] = y \n",
    "res.head ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Primero se visualiza el conjunto de datos en una dimensión: como una línea (no incluímos a PC2).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot (res ['PC1'], [0] * len (res), hue = res ['Y'], s = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Luego, se exponen los datos en un espacio 2D:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure (figsize = (10, 5)) \n",
    "sns.scatterplot(res['PC1'], res['PC2'], hue=res['Y'], s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En ambos gráficos se observa que las variables son difíciles de separar.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vemos cómo funciona el **modelo PCA**, con 2 componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los datos transformados se han reducido a una sola dimensión.*\n",
    "* *La reducción de dimensionalidad de PCA, eliminó la información a lo largo del eje o ejes principales menos importantes, dejando solo el componente o componentes de los datos con la mayor varianza, en éste caso, el primer y segundo componente.*\n",
    "* *Este conjunto de datos de dimensión reducida es, en algunos sentidos, \"lo suficientemente bueno\" para codificar las relaciones más importantes entre los puntos: a pesar de reducir la dimensión de los datos, la relación general entre los puntos de datos se conserva en su mayoría.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento del modelo implementado en la Entrega 01 - Árbol de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lista_rmse_train_dt = []\n",
    "lista_rmse_test_dt = []\n",
    "\n",
    "max_depths = [1,2,3,4,5,6,7,8,9,10,20,25,30,40,50,80]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "\n",
    "    # Se define el modelo con la profundidad deseada\n",
    "    tree_regressor = DecisionTreeRegressor(max_depth = max_depth, random_state=10)\n",
    "    \n",
    "    tree_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = tree_regressor.predict(X_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    \n",
    "    y_test_pred = tree_regressor.predict(X_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    lista_rmse_train_dt.append(rmse_train)\n",
    "    lista_rmse_test_dt.append(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "plt.plot(max_depths, lista_rmse_train_dt,'o-',label='train' )\n",
    "plt.plot(max_depths, lista_rmse_test_dt,'o-',label='test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Max. Profundidad\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title('Curva de Validación - Árbol de Decisión')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresor = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "regresor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ['Árbol de Decisión-PCA']\n",
    "\n",
    "for i, model in enumerate([regresor]):\n",
    "    y_train_pred = model.predict(X_train).reshape(50358,1)\n",
    "    y_test_pred = model.predict(X_test).reshape(21582,1)\n",
    "    \n",
    "    print(f'Modelo: {modelo[i]}')\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 25, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 25, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.title('Histograma de Errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)    \n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  \n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  \n",
    "    ]\n",
    "    \n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "    plt.title('Gráfico de Dispersión')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Los RMSE tanto para Train como para Test, resultan mucho menores a los obtenidos en el Proyecto 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regresor.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La primer característica tiene una mayor importancia relativa a la hora de realizar predicciones (su importancia es del 88%).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluación del desempeño del modelo obtenido, luego de transformar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Cabe aclarar, que en la presente transformación de datos, incluímos el escalado de los mismos, lo cual no se realizó en el Proyecto 01.*\n",
    "* *Es por ello que:*\n",
    "    * *Con el fin de **comparar bajo las mismas escalas**: se procederá a replicar los modelos realizados en el Proyecto 01, aunque realizando el escalado de sus datos.*\n",
    "    * *Para sumar una comparación más: se contrastarán los valores originales obtenidos en el Proyecto 01 (sin escalado), con el del Presente Proyecto pero **sin escalar**, es decir, se aplicará el modelo de Árbol de Decisión a partir del dataset **properati_6** (habiendo **aplicado PCA**).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Repetimos los modelos aplicados en el Proyecto 01, realizando el Escalado de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Replicamos los Filtros del Primer Proyecto de Machine Learning (Proyecto 01) y Escalamos los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml = pd.read_csv('DS_Proyecto_01_Datos_Properati.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_2 = properati_ml [(properati_ml['l2'] == 'Capital Federal') & ((properati_ml['property_type'] == 'Departamento') | (properati_ml['property_type'] == 'PH') | (properati_ml['property_type'] == 'Casa'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_3 = properati_ml_2 [(properati_ml_2['surface_total'] >= 15) & (properati_ml_2['surface_total'] <= 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_4 = properati_ml_3 [(properati_ml_3['price'] <= 4000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_5 = properati_ml_4.loc[:, ['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_5.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_5.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Checkpoint:__ deberías obtener un dataset con 81019 instacias y 6 columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Cabe aclarar, que el Proyecto 01 tiene un **Error**, ya que si bien el Checkpoint dió OK, no fue aplicado el último filtro en un nuevo dataframe, por lo que el total de instancias con las que se trabajó, fue de **82.373**.*\n",
    "* *A fin de comparar, se utilizará dicho dataset para no modificar los resultados finales*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_scaler_ml = RobustScaler().fit_transform(properati_ml_5[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']])\n",
    "\n",
    "properati_scaler_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati_ml_5_sclaer = pd.DataFrame(properati_scaler_ml, index = properati_ml_5[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].index,\n",
    "                         columns = properati_ml_5[['rooms','bedrooms','bathrooms','surface_total','surface_covered','price']].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = properati_ml_5_sclaer[['rooms','surface_total']]\n",
    "y = properati_ml_5_sclaer[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_train, y_train)\n",
    "print(linear_model.coef_, linear_model.intercept_) # Pendiente y ordenada al origen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se realiza la evaluación del modelo.*\n",
    "* *Se realiza el `histograma de los errores` ( y−y_predicho ) para cada conjunto y el `gráfico de dispersión` de  y  vs  y_predicho  para el conjunto de test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ['Regresión lineal']\n",
    "\n",
    "for i, model in enumerate([linear_model]):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f'Modelo: {modelo[i]}')\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 25, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 25, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.title('Histograma de Errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)    \n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  \n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  \n",
    "    ]\n",
    "    \n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "    plt.title('Gráfico de Dispersión')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como se indicó en el proyecto anterior, si bien los histogramas de los errores parecieran ser algo simétricos, no se observan parecidos entre ambos conjuntos, siendo un indicador de que **no** nos estamos aproximando a nuestros datos de manera correcta.*\n",
    "* *En el gráfico, **y**  vs  **y predicho**  para el conjunto de test, los puntos se van alejando de la diagonal, y el error, parece ser cada vez mayor.*\n",
    "* *En éste caso, la regresión lineal no estaría haciendo un buen trabajo en reproducir la curva teórica.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3. Árboles de Decisión y K Vecinos Más Cercanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = properati_ml_5_sclaer[['rooms','surface_total']]\n",
    "y = properati_ml_5_sclaer[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor = DecisionTreeRegressor(max_depth=25, random_state=10)\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor.fit(X_train, y_train)\n",
    "knn_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = ['Árbol de Decisión', 'K Vecinos Más Cercanos']\n",
    "for i, model in enumerate([tree_regressor, knn_regressor]):\n",
    "    y_train_pred = model.predict(X_train).reshape(57661,1)\n",
    "    y_test_pred = model.predict(X_test).reshape(24712,1)\n",
    "    \n",
    "    print(f'Modelo: {modelos[i]}')\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 25, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 25, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.title('Histograma de Errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)    \n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "    ]\n",
    "    \n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "    plt.title('Gráfico de Dispersión')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Tanto con Árboles de Decisión como en K Vecinos, los histogramas de los errores no parecen ser simétricos ni parecidos para ambos conjuntos, siendo un indicador de que **no** nos estamos aproximando a nuestros datos de manera correcta.*\n",
    "* *En los gráficos, **y**  vs  **y predicho**  para el conjunto de test, los puntos se encuentran cada vez más dispersos, y el error, es ser cada vez mayor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Modelo de Árbol de Decisión a partir del dataset **properati_6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = properati_6[['surface_total','surface_total','bedrooms','rooms','bathrooms']]\n",
    "y = properati_6[['price']]\n",
    "\n",
    "features = X.T\n",
    "cov_matrix = np.cov(features)\n",
    "\n",
    "valores, vectores = np.linalg.eig (cov_matrix)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor = DecisionTreeRegressor(max_depth=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = tree_regressor.predict(X_train)\n",
    "y_test_pred = tree_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "print(f'Raíz del error cuadrático medio en Test: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluación del Desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                     Modelos Con Escalado de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Modelos Proyecto 01  | RMSE Train | RMSE Test | Hiperparámetros Utilizados |\n",
    "|:----------------------:|:----------:|:---------:|:--------------------------:|\n",
    "|    Regresión Lineal    |    1.377   |   1.443   |             --             |\n",
    "|    Árbol de Decisión   |    0.955   |   1.259   |       max_depth = 25       |\n",
    "| K Vecinos Más Cercanos |    1.131   |   1.265   |       n_neighbors = 8      |\n",
    "\n",
    "|    Modelo Proyecto 02   | RMSE Train | RMSE Test | Hiperparámetro Utilizado |\n",
    "|:-----------------------:|:----------:|:---------:|:------------------------:|\n",
    "| Árbol de Decisión (PCA) |    0.458   |   0.470   |      max_depth = 10      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                     Modelos Sin Escalado de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Modelos Proyecto 01  | RMSE Train | RMSE Test | Hiperparámetros Utilizados |\n",
    "|:----------------------:|:----------:|:---------:|:--------------------------:|\n",
    "|    Regresión Lineal    |  213500.2  |  223733.9 |             --             |\n",
    "|    Árbol de Decisión   |  148049.8  |  195382.0 |       max_depth = 25       |\n",
    "| K Vecinos Más Cercanos |  174201.2  |  197995.1 |       n_neighbors = 8      |\n",
    "\n",
    "|    Modelo Proyecto 02   | RMSE Train | RMSE Test | Hiperparámetro Utilizado |\n",
    "|:-----------------------:|:----------:|:---------:|:------------------------:|\n",
    "| Árbol de Decisión (PCA) |   54696.3  |  56087.7  |      max_depth = 10      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Según se expone, tanto en el caso de modelos escalados como no escalados, **el Desempeño del Modelo Obtenido del Proyecto 02, es mucho mejor** que los extrtraídos del proyecto anterior.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En los 3 modelos aplicados en el Proyecto 01, los errores RMSE, tienen valores muy altos, por lo que no logran una buena predicción de los precios.*\n",
    "* *Ésto ocurre, porque los errores RMSE, son sensibles a valores atípicos, y en la muestra del Proyecto 01, tenemos varios, ya que en su oportunidad, no se realizó el filtrado de datos más adecuado para reducir los Outliers de la mejor manera posible, ni tampoco imputación de datos o reducción de dimensionalidad*.\n",
    "* *Además, hay un notable sobreajuste en los modelos ejecutados en el Proyecto 01, visualizado a través de la importante diferencia existente entre el RMSE del Train y del Test (el primero mucho menor).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE B - Modelos Avanzados ##\n",
    "\n",
    "* Elige dos de los modelos avanzados vistos (en el caso de regresión, considera una regresión lineal con atributos polinómicos y regularización). Entrénalos y evalúalos con sus argumentos por defecto. No te olvides de hacer un train/test split y usar Validación Cruzada.\n",
    "\n",
    "* Optimiza sus hiperparámetros mediante Validación Cruzada y Grid Search o Random Search.\n",
    "\n",
    "* Compara el desempeño de los nuevos modelos entre sí y con el modelo de la Parte A. ¿Cuál elegirías? Justifica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Aclaración:__ Los clasificadores basados en modelos gráficos, como Fisher LDA o Naive Bayes, así como los Árboles de decisión y los métodos de conjuntos basados en Árboles (RF, XGB) son invariantes al escalado de características (no lo requieren para converger a los errores mínimos), pero aún así, podría ser una buena idea reescalar / estandarizar su datos.  \n",
    "Es por ello, que en el presente apartado, se optó por elegir el dataset **properati_7**, el cual incluye el escalado de los datos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regresión Lineal con Atributos Polinómicos (Polynomial Features) y Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Regresión Lineal con Atributos Polinómicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La regresión polinomial es otra forma de regresión en la que la potencia máxima de la variable independiente es más de 1. En esta técnica de regresión, la línea de mejor ajuste no es una línea recta, sino que tiene la forma de una curva.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = properati_7[['bedrooms','surface_total','surface_covered']]\n",
    "y = properati_7[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_regresion(model,X,y, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculamos el Error\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "\n",
    "    plt.figure(figsize = (17,5))\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 20, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 20, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,3,3)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)\n",
    "\n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n",
    "    ]\n",
    "\n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos cuál es el mejor Grado (Degree) a aplicar\n",
    "rmses = []\n",
    "degrees = np.arange(1, 10)\n",
    "min_rmse, min_deg = 1e10, 0\n",
    "\n",
    "for deg in degrees:\n",
    "\n",
    "    poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "    X_poly_train = poly_features.fit_transform(X_train)\n",
    "\n",
    "    poly_reg = LinearRegression()\n",
    "    poly_reg.fit(X_poly_train, y_train)\n",
    "\n",
    "    X_poly_test = poly_features.fit_transform(X_test)\n",
    "    poly_predict = poly_reg.predict(X_poly_test)\n",
    "    poly_mse = mean_squared_error(y_test, poly_predict)\n",
    "    poly_rmse = np.sqrt(poly_mse)\n",
    "    rmses.append(poly_rmse)\n",
    "    \n",
    "    if min_rmse > poly_rmse:\n",
    "        min_rmse = poly_rmse\n",
    "        min_deg = deg\n",
    "\n",
    "print('Best degree {} with RMSE {}'.format(min_deg, min_rmse))\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(degrees, rmses)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('RMSE')\n",
    "plt.title('Degree Vs. RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Si bien el gráfico expone un punto mínimo en Grado = 5, parece ser una línea bastante recta desde Grado = 3 hasta dicho punto.*\n",
    "* *Por ello, y dado que nuestro dataset tiene varios atributos, el modelo se llevará a cabo con un polinomio de grado 3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures (3, include_bias = False) # Preprocesamiento, de 'Grado 3'\n",
    "X_train_new = poly.fit_transform(X_train) # Conjunto de datos que incluye los nuevos atributos, las nueva columnas\n",
    "X_test_new = poly.fit_transform(X_test)\n",
    "\n",
    "# Ahora son 3 columnas. Son nuevas combinaciones lineales del atributo original.\n",
    "# A la vista, elevó a la 1, a la 2, a la 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train_new, y_train)\n",
    "print(f'Pendientes: {reg.coef_}')\n",
    "print(f'Ordenada: {reg.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_reg = reg.predict(X_train_new)\n",
    "y_test_pred_reg = reg.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluar_regresion(reg, X,y, X_train_new, X_test_new, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En la regularización, lo que hacemos es mantener el mismo número de características, pero reducir la magnitud de los coeficientes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Regulación L2 o RIDGE:__ Se agrega a la función de Costo, un término proporcional al cuadrado del valor de los coeficientes de peso. La penalización, encoge los coeficientes hacia el Cero (pero no llegan al cero absoluto).*\n",
    "    * *El valor de Alpha, puede ser de 0.1 hasta el valor que se desee.*\n",
    "    * *Cuanto mayor sea el valor de alfa, menos varianza exhibirá su modelo y menor será la dispersión de los datos.*\n",
    "    * *Funciona bien si hay muchos parámetros grandes de aproximadamente el mismo valor.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Regulación L1 o LASSO:__ Se agrega a la función de Costo, un término proporcional al valor absoluto de los coeficientes de peso. La penalización, encoge los coeficientes hacia el Cero o convierte a algunos coeficientes en Cero. Así, elimina las características menos importantes en nuestro modelo.*\n",
    "    * *El valor de Alpha, puede variar de 0.1 a 1.*\n",
    "    * *Tiende a funcionar bien si hay una pequeña cantidad de parámetros significativos y los otros están cerca de cero.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1 Se entrena el modelo **Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiVHWZ6nKbhS"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "reg_ridge = Ridge() # No se incluye alpha, ya que en la consigna se indica entrenar y evaluar con los argumentos por defecto\n",
    "reg_ridge.fit(X_train_new,y_train)\n",
    "\n",
    "print(f'Pendientes: {reg_ridge.coef_}')\n",
    "print(f'Ordenada: {reg_ridge.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3vL2DC9zf4g"
   },
   "outputs": [],
   "source": [
    "evaluar_regresion(reg_ridge, X,y, X_train_new, X_test_new, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Se entrena el modelo **Lasso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ib8_lho-zf4k"
   },
   "outputs": [],
   "source": [
    "reg_lasso = Lasso()\n",
    "reg_lasso.fit(X_train_new,y_train)\n",
    "\n",
    "print(reg_lasso.coef_, reg_lasso.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_regresion_2(model,X,y, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    y_train_pred = model.predict(X_train).reshape(50358,1)\n",
    "    y_test_pred = model.predict(X_test).reshape(21582,1)\n",
    "    \n",
    "    # Calculamos el Error\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "\n",
    "    ### Graficamos los Resultados\n",
    "    plt.figure(figsize = (17,5))\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 20, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 20, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,3,3)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)\n",
    "\n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n",
    "    ]\n",
    "\n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "evaluar_regresion_2(reg_lasso, X,y, X_train_new, X_test_new, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *La regularización __`Ridge` resultó ser más efectiva__ que Lasso, y ésto es porque la mayoría de los predictores elegidos influyen en la respuesta (superficie total y cubierta influyen sobre la determinación de los precios).*\n",
    "* *El modelo con regularización __`Ridge`__, se observa una distribución simétrica y más si la comparamos con Lasso.* \n",
    "* *Además, a simple vista se observa que para el caso de Lasso, en el gráfico **y**  vs  **y predicho**  para el conjunto de test, los puntos se van alejando de la diagonal, y el error, parece ser cada vez mayor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Optimización de Hiperparámetros: RandomSerch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.1 Se determina el mejor `alpha` para el modelo Ridge, por ser el que mejor resultado arrojó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scipy as sp\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV # Búsqueda aleatoria en hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_ramdom = dict(alpha=alpha)\n",
    "ramdom = RandomizedSearchCV(estimator=ridge, param_distributions=param_ramdom, verbose=1, n_jobs=-1, n_iter = 100)\n",
    "ramdom_result = ramdom.fit(X_train_new, y_train)\n",
    "print('Best Score: ', ramdom_result.best_score_)\n",
    "print('Best Params: ', ramdom_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2 Se entrena el modelo Ridge con el `Best Params` según RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El resultado obtenido, debería ser el mismo que en el modelo Ridge anteriormente entrenado y evaluado, ya que el alpha por default es 1.0, que es el resultante en éste caso como Best Params.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge_random = Ridge(alpha= 0.001)\n",
    "reg_ridge_random.fit(X_train_new,y_train)\n",
    "\n",
    "print(f'Pendientes: {reg_ridge.coef_}')\n",
    "print(f'Ordenada: {reg_ridge.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_random = reg_ridge_random.predict(X_train_new)\n",
    "y_test_pred_random = reg_ridge_random.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred_random))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred_random))\n",
    "print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "print(f'Raíz del error cuadrático medio en Test: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se confirma que los RMSE obtenidos son los mismos.*\n",
    "* *Para el caso de Polynomial Features, el similar el resultado del modelo antes y después de efectuada la Regularización.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = properati_7[['bedrooms','surface_total','surface_covered']]\n",
    "y = properati_7[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Se entrena y evalúa el modelo, con sus argumentos por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los datos en train y test (held-out)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgb.XGBRegressor() # Default n_estimators=100, max_depth=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train, eval_metric=['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_xgb = xgb.predict(X_train)\n",
    "y_test_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n",
    "print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "print(f'Raíz del error cuadrático medio en Test: {rmse_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *`Surface Total` es el atributo mas importante, el que mejor separa los datos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Optimización de Hiperparámetros: GridSerch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor()\n",
    "n_estimators = [50,100,150,200,250,300]\n",
    "max_depth = [2,4,6,8]\n",
    "\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=kfold, verbose=1)\n",
    "grid_result = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Se entrena el modelo XGB con los argumentos obtenidos de Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = xgb.XGBRegressor(n_estimators=100, max_depth=8, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X_train, y_train, eval_metric=['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ['XG-Boost']\n",
    "\n",
    "for i, model in enumerate([xgb_grid]):\n",
    "    y_train_pred = model.predict(X_train).reshape(50358,1)\n",
    "    y_test_pred = model.predict(X_test).reshape(21582,1)\n",
    "    \n",
    "    print(f'Modelo: {modelo[i]}')\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    print(f'Raíz del error cuadrático medio en Train: {rmse_train}')\n",
    "    print(f'Raíz del error cuadrático medio en Test: {rmse_test}')\n",
    "    \n",
    "    plt.figure(figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot(y_train - y_train_pred, bins = 25, label = 'train')\n",
    "    sns.distplot(y_test - y_test_pred, bins = 25, label = 'test')\n",
    "    plt.xlabel('errores')\n",
    "    plt.title('Histograma de Errores')\n",
    "    plt.legend()\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    ax.scatter(y_test,y_test_pred, s =2)    \n",
    "    lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  \n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  \n",
    "    ]\n",
    "    \n",
    "    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.xlabel('y (test)')\n",
    "    plt.ylabel('y_pred (test)')\n",
    "    plt.title('Gráfico de Dispersión')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_grid.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Si bien `Surface Total` sigue siendo el atributo que mejor separa los datos, `Surface Total` ha adquirido importancia mayor importancia.*\n",
    "* *Habiendo definido los Hiperparámetros, el Error de Train mejoró (se redujo en 0.03), y algo menos el Error de Test (se redujo en 0.01).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparación del desempeño de los nuevos modelos entre sí y con el modelo de la Parte A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      Modelo Parte A     | RMSE Train | RMSE Test | Hiperparámetro Utilizado |\n",
    "|:-----------------------:|:----------:|:---------:|:------------------------:|\n",
    "| Árbol de Decisión (PCA) |    0.458   |   0.470   |      max_depth = 10      |\n",
    "\n",
    "\n",
    "|     Modelos Parte B    | RMSE Train | RMSE Test |   Hiperparámetros Utilizados  |\n",
    "|:----------------------:|:----------:|:---------:|:-----------------------------:|\n",
    "|   Polynomial Features  |    0.480   |   0.480   |  alpha= 1.0, PolyFeatures = 3 |\n",
    "|        XG-Boost        |    0.375   |   0.424   | n_estimators=100, max_depth=8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En éste caso, fue **XG-Boost fue el modelo con mejor desempeño**.*\n",
    "* *Éste resultado, era realmente el esperado, ya que se trata de un algoritmo eficiente y fácil de usar que ofrece un alto rendimiento y precisión en comparación con otros algoritmos.*\n",
    "* *Además, XGBoost tiene una regularización incorporada L1 (Regresión de lazo) y L2 (Regresión de cresta) que evita que el modelo se sobreajuste.*\n",
    "* *XGBoost, por otro lado, hace divisiones hasta el max_depth especificado y luego comienza a podar el árbol hacia atrás y elimine las divisiones más allá de las cuales no hay ganancia positiva.*\n",
    "* *Es por ello que se considera uno de los modelos más eficientes a la hora de realizar predicciones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El modelo de **Árbol de Decisión habiendo aplicado PCA**, posee igualmente, una buena capacidad de predicción, por el hecho de haber realizado reducción de dimensionalidad, pasando de 5 features a 2 estimadores.*\n",
    "* *No se observa en éste caso Sobrejuste, ya que los RMSE obtenidos para Train y Test son similares, existiendo un equilibrio entre sesgo y varianza.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En el caso de **Polynomial Features**, lo que se hizo fue crear muchos atributos nuevos con combinaciones distintas, y esperar aproximarse con esas combinaciones, a la función objetivo.*\n",
    "* *El modelo se desarrolló con PolynomialFeatures de Grado 3, ya que si bien mayores grados podría haber dado lugar a un mejor desempeño, no era muy recomendable dado el número de features en nuestro dataset, y además porque su desempeño no parecía variar entre dicho Grado y el indicado como Óptimo (Grado = 5).*\n",
    "* *No se observó sobreajuste en el modelo, ya que se llevó adelante la regularización a través de L2 Ridge, sin embargo no se obtuvieron tan buenos resultados como en el caso de Arboles de Decisión previo PCA*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTE C - Interpretación de modelos ##\n",
    "\n",
    "De acuerdo a lo que el modelo permite, responde algunas o todas las siguientes preguntas:\n",
    "\n",
    "* ¿Qué variables fueron relevantes para el modelo para hacer una predicción? ¿Cuáles no? Si usaste una regresión lineal con regularización, presta atención a los parámetros (pendientes) obtenidas. Si usaste un modelo de ensamble en árboles, además de ver la importancia de cada atributo, también elige algunos árboles al azar y observa qué atributos considera importantes. ¿En qué se diferencian esos árboles? ¿Por qué? Finalmente, responde, ¿coincide con lo que esperabas a partir de tu experiencia con este dataset?\n",
    "\n",
    "* ¿Cómo es la distribución de errores (regresión) o qué clases se confunden entre sí (clasificación)? ¿Dónde falla? ¿A qué se debe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables:\n",
    "* *La variable más relevante a la hora de hacer predicciones, fue `Superficie Total`, y en algunos casos, como segunda más importante, `Superficie Cubierta`.*\n",
    "* *Las variables menos importantes y por ello no utilizadas para la mayoría de los modelos, fueron `bathrooms` y `rooms`.*\n",
    "* *Cabe aclarar que, antes de determinar cuáles fueron las variables que no se utilizarían en los modelos, se realizaron pruebas con las mismas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pendiente Obtenida en Regresión Polinómica con Regularización:\n",
    "* *El target es el valor que desea predecir (y = precio). Entonces, como la Regresión Ridge (elegida) puede predecir más valores para cada instancia (no solo uno), lo que va a mostrar **coef_** es los coeficientes para la predicción de cada uno de dichos targets.*\n",
    "* *En este caso, el coeficiente es la pendiente de la línea ajustada y la intersección es el punto donde la línea ajustada se cruza con el eje y.*\n",
    "* *Como se observa, en el gráfico **y**  vs  **y predicho**  para el conjunto de test, los puntos si bien comienzan desde una distancia relativa de la diagonal (pendiente), se van alejando algo más de la misma, y el error, parece ser cada vez mayor.*\n",
    "* *Igualmente su desempeño es notablemente mejor que para el caso de Regresión Lineal, que comienza con sus puntos coincidentes respecto a la pendiente y luego se separan progresivamente.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribución de los Errores:\n",
    "* *Es en el gráfico de error de XG-Boost, donde se visualiza una mayor robustez en los datos.*\n",
    "* *Asimismo, en el gráfico, **y**  vs  **y predicho** para el conjunto de test, para el mismo modelo, si bien los puntos se van alejando de la diagonal, el error parece mantenerse en los mismos rangos relativos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESAFÍO OPCIONAL ##\n",
    "\n",
    "Aplica una técnica de Clustering sobre el dataset. Puedes combinar con técnicas de reducción de dimensionalidad para facilitar la visualización. ¿Qué clusters encuentras? ¿A qué pueden corresponder? Te dejamos preguntas que pueden servir como disparadoras: ¿qué barrios se parecen más entre sí?¿qué tipos de propiedades se parecen más entre sí?\n",
    "\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.datasets import make_blobs, make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se utiliza el dataset **properati_6**, ya que incluye el feature `l3`, que es sobre el cual se trabajará para determinar los Clusters existentes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Se aplica **Dummies** sobre la variable `Barrios`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(properati_6['l3'])\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Dado que ha sido creado un marco de datos completamente nuevo, y para compararlo con el marco de datos original, es necesario fusionarlos o concatenarlos para que funcionen correctamente. Al crear variables ficticias, se crean nuevas columnas para el conjunto de datos original.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se coloca la variable ficticia en el lado derecho del marco de datos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrios = pd.concat([properati_6,dummies], axis = 1)\n",
    "barrios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = barrios[['l3']]\n",
    "y = barrios[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Se determina el **número de Centros**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 71940\n",
    "n_centros = 8\n",
    "X, y = make_blobs(centers=n_centros, cluster_std=0.3, n_features=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se grafican los datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = X[:,0], y = X[:,1], hue = y, legend ='full')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se evaluarán las particiones a realizar mediante KMeans usando la distancia al centroide. La idea es que al variar el número de clúster K en el modelo, el valor de la distancia media de los datos al centroide más cercano va a variar.*\n",
    "* *Se grafica dicha curva, para elegir el número de particiones óptimos con el metodo del codo.*\n",
    "* *Se realiza una lista con las distancias medias a los centroides en el dataset 1, probando con un número de clústers que va de 2 a 14.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_distancias_medias = []\n",
    "\n",
    "K = np.arange(2,14)\n",
    "for k in K:\n",
    "   \n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "   \n",
    "    distancia_total = km.inertia_\n",
    "    # Inercia: la suma de la distancia al cuadrado de cada punto con su respectivo centroide\n",
    "    \n",
    "    distancia_media = distancia_total / n_samples\n",
    "    lista_distancias_medias.append(distancia_media)\n",
    "    # Distorsión: el promedio de todas las distancias de los centroides con sus respectivos puntos al cuadrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se grafica la distancia media en función del número de clústers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "# Graficamos una linea continua y tambien unos puntos para resaltar los valores enteros de K.\n",
    "plt.plot(K, lista_distancias_medias, lw=3)\n",
    "plt.scatter(K, lista_distancias_medias, s=55,c='r')\n",
    "plt.xlabel('Cantidad de Clusters K')\n",
    "plt.ylabel('Inercia media')\n",
    "plt.title('Método del Codo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *S/ el gráfico, parecería que el n_clusters más acorde sería de 6.*\n",
    "* *Se busca el mejor numero de k en cada caso según la curva y se grafican los clusters para cada dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino y entreno el modelo\n",
    "km = KMeans(n_clusters=6)\n",
    "km = km.fit(X)\n",
    "\n",
    "# Obtengo la posición de los centros y las etiquetas\n",
    "etiquetas_ = km.labels_\n",
    "centros_ = km.cluster_centers_\n",
    "\n",
    "# Graficamos los centros de los clusters y los datapoints\n",
    "sns.scatterplot(X[:, 0], X[:, -1], style = etiquetas_)\n",
    "sns.scatterplot(centros_[:, 0], centros_[:, 1],color='black', marker=\"+\", s=1000)\n",
    "plt.title('Data points y Centroides de los Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Silhouette\n",
    "\n",
    "* *A continuación, se evalúan las particiones mediante el valor de silhouette.*\n",
    "* *Al variar los parámetros de los modelos de clustering, cambiará la distribución del valor de Silhouettes en los datos. Con esa distribución, se elegirán los mejores parametros posibles (cohesión y separación).*\n",
    "\n",
    "*__Nota 1:__ el coeficiente de Silhoutte va de -1 a 1.*\n",
    "\n",
    "*__Nota 2:__ el Silhouette promedio también va del -1 al 1, donde en 1 los clusters están bien separados, en 0 están cerca y en -1 el los clusters están mezclados. Esto nos da una idea de cuán buena es la separación en clusters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *En ésta oportunidad, vamos a calcular el valor de silhouette usando la función `silhouette_score`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos una lista donde vamos a ir agregando los valores medios de silhouette\n",
    "lista_sil = []\n",
    "# Fiteammos un modelo para cada numero de cluster que queremos testear\n",
    "for k in range(2,14):\n",
    "    # Definimos y entrenamos el modelo\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(X)\n",
    "    \n",
    "    # Tomamos las etiquetas\n",
    "    etiquetas = km.labels_\n",
    "    \n",
    "    # Calculamos el silhouette \n",
    "    valor_medio_sil = silhouette_score(X, etiquetas)\n",
    "    lista_sil.append(valor_medio_sil)\n",
    "    \n",
    "plt.figure(figsize = (10,7))\n",
    "plt.plot(K, lista_sil, lw=3)\n",
    "plt.scatter(K, lista_sil,s=55,c='r')\n",
    "plt.xlabel('Cantidad de Clusters K')\n",
    "plt.ylabel('Silhouette Media')\n",
    "plt.title('Silhouette Media por Nro. de Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *El gráfico, representa el Promedio de todas las siluetas por cantidad de clusters.*\n",
    "* *Si contrastamos con la próxima gráfica de silueta, que se verá de costado, vendría a ser la línea punteada observada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = X\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "    \n",
    "km = KMeans(n_clusters=8)\n",
    "labels = km.fit_predict(X_std)\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "silhouette_vals = silhouette_samples(X_std, labels)\n",
    "\n",
    "y_ticks = []\n",
    "y_lower, y_upper = 0, 0\n",
    "for i, cluster in enumerate(np.unique(labels)):\n",
    "    cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    y_upper += len(cluster_silhouette_vals)\n",
    "    ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "    ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "    y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "avg_score = np.mean(silhouette_vals)\n",
    "ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "ax1.set_xlabel('Valores del Coeficiente de Silueta')\n",
    "ax1.set_ylabel('Clusters')\n",
    "ax1.set_title('Diagrama de Siluetas', y=1.02);\n",
    "\n",
    "# Scatter plot of data colored with labels\n",
    "ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)\n",
    "ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n",
    "ax2.set_xlim([-12, 12])\n",
    "ax2.set_xlim([-12, 12])\n",
    "ax2.set_xlabel('Tiempo de Emisión en Minutos')\n",
    "ax2.set_ylabel('Tiempo de Espera p/ Próxima Emisión')\n",
    "ax2.set_title('Visualización de los Clusters', y=1.02)\n",
    "ax2.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Análisis de Silueta utilizando k = {6}', fontsize=16, fontweight='semibold', y=1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Para entender mejor el gráfico:*\n",
    "    * *En el eje x está el valor de Silhouette, en el eje y los clusters.*\n",
    "    * *El valor de silhouette de cada instancia está graficado como una barrita muy finita. Están todas las instancias graficadas, ordenadas de mayor a menor.*\n",
    "    * *la línea punteada roja representa el Silhouette promedio de toda la partición.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como se observa, el mejor número de Clusters para separar los datos es **k=8**.*\n",
    "* *Sin embargo, si bien sabemos que los barrios se encuentran dividimos en 6 grupos, no tenemos certeza de cuales de ellos pertenecen a cada grupo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se importan los módulos de Python necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Se eliminan las coordenadas nulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barrios.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = barrios.dropna() # Se eliminan todos los núlos porque sólo se trata de lat y lon (variables que vamos a utilizar)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Se convierten las columnas de coordenadas de latitud y longitud en una matriz numérica bidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df[['lat', 'lon']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Se **calcula DBSCAN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Se utiliza la métrica de Haversine y el algoritmo de Ball Tree, para calcular distancias de grandes círculos entre puntos.*\n",
    "* *El parámetro **épsilon** es la distancia máxima (0,5 km en este ejemplo, por tratarse de barrios) que los puntos pueden estar entre sí para ser considerados un grupo.*\n",
    "* *El parámetro **min_samples** es el tamaño mínimo del clúster (todo lo demás se clasifica como ruido). Al ser min_samples=1, cada punto de datos será asignado a un grupo.*\n",
    "* *A diferencia de k-means, DBSCAN no requiere que especifique el número de clústeres por adelantado; los determina automáticamente en función de los parámetros épsilon y min_samples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kms_per_radian = 6371.0088\n",
    "epsilon = 0.5 / kms_per_radian\n",
    "db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n",
    "num_clusters = len(set(cluster_labels))\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "print('Number of clusters: {}'.format(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# define a helper function to get the colors for different clusters\n",
    "def get_cmap(N):\n",
    "    '''\n",
    "    Returns a function that maps each index in 0, 1, ... N-1 to a distinct \n",
    "    RGB color.\n",
    "    '''\n",
    "    color_norm  = colors.Normalize(vmin=0, vmax=N-1)\n",
    "    scalar_map = cmx.ScalarMappable(norm=color_norm, cmap='nipy_spectral') \n",
    "    def map_index_to_rgb_color(index):\n",
    "        return scalar_map.to_rgba(index)\n",
    "    return map_index_to_rgb_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 12))\n",
    "m = Basemap(projection='merc', resolution='l', epsg = 4269, \n",
    "        llcrnrlon=-122.7,llcrnrlat=36.2, urcrnrlon=-120.8,urcrnrlat=37.5)\n",
    "\n",
    "unique_label = np.unique(cluster_labels)\n",
    "\n",
    "# get different color for different cluster\n",
    "cmaps = get_cmap(n_clusters)\n",
    "\n",
    "# plot different clusters on map, note that the black dots are \n",
    "# outliers that not belone to any cluster. \n",
    "for i, cluster in enumerate(clusters):\n",
    "    lons_select = cluster[:, 1]\n",
    "    lats_select = cluster[:, 0]\n",
    "    x, y = m(lons_select, lats_select)\n",
    "    m.scatter(x,y,5,marker='o',color=cmaps(i), zorder = 10)\n",
    "\n",
    "m.arcgisimage(service='World_Shaded_Relief', xpixels = 5000, verbose= False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Como vemos, coincide con el número de Clusters arrojados por Silhouette.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Se busca encontrar el **punto más central** de un grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "centermost_points = clusters.map(get_centermost_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = rep_points.apply(lambda row : df[(df['lat']==row['lat']) & (df['lon']==row['lon'])].iloc[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the location of the earthquakes\n",
    "plt.figure(figsize = (12, 12))\n",
    "\n",
    "m = Basemap(projection='merc', resolution='l', epsg = 4269, \n",
    "            llcrnrlon=-122.7,llcrnrlat=36.2, urcrnrlon=-120.8,urcrnrlat=37.5)\n",
    "\n",
    "# plot the aftershock\n",
    "x, y = m(coords[:, 1], coords[:, 0])\n",
    "m.scatter(x,y,5,marker='o',color='b')\n",
    "m.arcgisimage(service='World_Shaded_Relief', xpixels = 5000, verbose= False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "rs_scatter = ax.scatter(rs['lon'], rs['lat'], c='#99cc99', edgecolor='None', alpha=0.7, s=120)\n",
    "df_scatter = ax.scatter(df['lon'], df['lat'], c='k', alpha=0.9, s=3)\n",
    "ax.set_title('Full data set vs DBSCAN reduced set')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend([df_scatter, rs_scatter], ['Full set', 'Reduced set'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
